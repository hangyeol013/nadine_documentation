{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#nadine-project-documentation","title":"Nadine Project Documentation","text":"<p>This site documents the main layers of the Nadine social robot system and is aimed at new developers joining the project.</p> <ul> <li>Perception layer: camera input, face detection and recognition, 3D tracking, and selective visual memory.</li> <li>Interaction layer: dialogue, memory, and reasoning components.</li> <li>Control layer: low-level robot control, animations, and hardware integration.</li> </ul> <p>Each layer page explains:</p> <ul> <li>What the layer does and how it fits in the overall system</li> <li>How to run it end-to-end</li> <li>Important modules, their responsibilities, and how they interact</li> <li>Key configuration options and typical extension points</li> </ul>"},{"location":"control-overview/","title":"Control Layer \u2013 Overview","text":"<p>The control layer drives Nadine\u2019s physical embodiment: head/eye pose, gestures, and speech (audio + lip movements).</p> <p>If you are new to the project, start with Project Overview, then use this page to understand what the control component does and how to run it.</p>"},{"location":"control-overview/#responsibilities","title":"Responsibilities","text":"<ul> <li>Receive high\u2011level commands via MQTT from interaction/perception:  </li> <li>speak, look_at, play animation</li> <li>Execute joint\u2011level motion using XML animation files and per\u2011joint trajectories</li> <li>Generate speech audio and lip movements using Azure Text-to-Speech and a lip animation generator</li> <li>Send feedback events (start/end speaking) back to the interaction layer</li> </ul>"},{"location":"control-overview/#files-and-modules","title":"Files and Modules","text":"<p>Main files under <code>control/</code>:</p> <ul> <li><code>main.py</code> \u2013 entrypoint; loads config, parses CLI arguments, and starts <code>NadineServer</code>.</li> <li><code>config.yaml</code> \u2013 default paths for voice data and animation XMLs.</li> <li><code>run.sh</code> \u2013 convenience script to activate the <code>nadine_new</code> env and run <code>main.py</code>.</li> </ul> <p>Key modules in <code>nadine/control/</code>:</p> <ul> <li><code>NadineServer.py</code> \u2013 owns the MQTT client, subscribes to control topics, and dispatches messages to <code>AgentControlHandler</code>.</li> <li><code>AgentControlHandler.py</code> \u2013 high\u2011level adapter that maps commands (look_at, speak, playAnimation) to <code>NadineControl</code> methods.</li> <li><code>NadineControl.py</code> \u2013 core robot controller; loads animation library, initializes joints, manages idle movements, and coordinates TTS + lip animation.</li> <li><code>Animations.py</code> \u2013 <code>AnimationLibrary</code> for loading/querying animation sequences from XML.</li> <li><code>AzureTTS.py</code> \u2013 Azure Text\u2011to\u2011Speech integration and lip animation generation.</li> <li><code>LipAnimationGenerator.py</code> \u2013 converts audio/phoneme data into mouth/jaw trajectories.</li> <li><code>Joint.py</code> \u2013 simple joint model for servo trajectories.</li> <li><code>SerialComm.py</code>, <code>Checker.py</code>, <code>StructDef.py</code> \u2013 low\u2011level communication and playback protocol with Nadine\u2019s motion controller.</li> <li><code>XMLAnimations/</code> \u2013 XML animation scenes defining gestures and postures.</li> </ul>"},{"location":"control-overview/#how-to-run","title":"How to Run","text":""},{"location":"control-overview/#prerequisites","title":"Prerequisites","text":"<ul> <li>Environment: <code>nadine_new</code> conda environment (from <code>control/environment.yml</code>).  </li> <li>Hardware:</li> <li>Nadine\u2019s motion controller connected via serial (port configured in <code>SerialComm</code>/<code>Checker</code>).</li> <li>Speakers connected to the control machine.  </li> <li>Services: MQTT broker at <code>localhost</code> or <code>emqx</code>.  </li> <li>Paths:</li> <li><code>control/config.yaml</code> defines default voice and animation paths.</li> </ul>"},{"location":"control-overview/#start-command","title":"Start command","text":"<p>From the control directory:</p> <pre><code>cd /home/miralab/Development/nadine_Jan_2026/control\nconda activate nadine_new\n./run.sh\n</code></pre> <p>or:</p> <pre><code>cd /home/miralab/Development/nadine_Jan_2026/control\nconda activate nadine_new\npython3 main.py\n</code></pre> <p>You can override paths on the command line:</p> <pre><code>python3 main.py \\\n  -voicepath &lt;path_to_default_voice&gt; \\\n  -voicepathGerman &lt;path_to_german_voice&gt; \\\n  -voicepathFrench &lt;path_to_french_voice&gt; \\\n  -animationXMLPath XMLAnimations\n</code></pre>"},{"location":"control-overview/#configuration-controlconfigyaml","title":"Configuration (<code>control/config.yaml</code>)","text":"<p>Control\u2011layer configuration lives under the <code>control:</code> key:</p> <ul> <li><code>voice</code></li> <li><code>default_path</code> \u2013 default (e.g. English) voice data.</li> <li><code>german_path</code> \u2013 German voice data.</li> <li> <p><code>french_path</code> \u2013 French voice data.</p> </li> <li> <p><code>animations</code></p> </li> <li><code>animation_xml_path</code> \u2013 directory containing animation XML files (relative to <code>control/</code> or absolute).</li> </ul> <p><code>main.py</code> loads this via <code>load_control_config()</code> and passes:</p> <ul> <li>Voice paths to <code>NadineServer</code> </li> <li>Animation XML path to <code>AgentControlHandler</code> \u2192 <code>NadineControl.load_animation_library(...)</code></li> </ul>"},{"location":"control-overview/#where-to-go-next","title":"Where to Go Next","text":"<ul> <li>See Control Layer / Runtime &amp; MQTT for details on how MQTT commands are turned into motions and speech.  </li> <li>Use the Project Overview page for how control interacts with the other layers.</li> </ul>"},{"location":"control-runtime/","title":"Control Layer \u2013 Runtime &amp; MQTT","text":"<p>This page explains how the control server processes MQTT commands and turns them into robot motions and speech.</p>"},{"location":"control-runtime/#entrypoint-mainpy","title":"Entrypoint (<code>main.py</code>)","text":"<p><code>main.py</code> is responsible for:</p> <ul> <li>Loading environment variables from <code>control/.env</code> (Azure TTS, etc.).  </li> <li>Loading <code>control/config.yaml</code> via <code>load_control_config()</code>.  </li> <li>Parsing optional CLI arguments:</li> <li><code>-voicepath</code>, <code>-voicepathGerman</code>, <code>-voicepathFrench</code>, <code>-animationXMLPath</code>.  </li> <li>Creating a <code>NadineServer</code> instance with the resolved voice paths.  </li> <li>Setting the animation XML path on the server.  </li> <li>Calling <code>start_server()</code> to begin handling MQTT commands.</li> </ul> <p>Once started, the process runs until you stop it (CTRL+C).</p>"},{"location":"control-runtime/#mqtt-server-nadineserver","title":"MQTT Server (<code>NadineServer</code>)","text":"<p><code>NadineServer</code> owns the MQTT client and bridges topics to robot actions.</p>"},{"location":"control-runtime/#setup","title":"Setup","text":"<ul> <li>Creates a <code>paho.mqtt.client.Client</code>.  </li> <li>Registers:</li> <li><code>on_connect</code> \u2013 called when the client connects.  </li> <li><code>on_message</code> \u2013 called when messages are received.  </li> <li>Attempts to connect to an MQTT broker:</li> <li>First <code>localhost:1883</code>, then falls back to <code>emqx:1883</code>.</li> </ul>"},{"location":"control-runtime/#subscriptions","title":"Subscriptions","text":"<p>On connect, the server subscribes to:</p> <ul> <li><code>nadine/agent/control/#</code></li> </ul> <p>This wildcard covers:</p> <ul> <li><code>nadine/agent/control/speak</code> </li> <li><code>nadine/agent/control/look_at</code> </li> <li><code>nadine/agent/control/animation</code></li> </ul>"},{"location":"control-runtime/#agentcontrolhandler-creation","title":"AgentControlHandler creation","text":"<p>When <code>start_server()</code> is called:</p> <ul> <li>Logs that the MQTT handler is starting.  </li> <li>Creates <code>AgentControlHandler(animation_xml_path)</code>.  </li> <li>Attaches itself to the robot:</li> <li><code>agentcontrol_handler.robot.nadine_server = self</code></li> </ul> <p>This allows <code>NadineControl</code> to send feedback (<code>start_speak</code>, <code>end_speak</code>) back through the server.</p>"},{"location":"control-runtime/#message-handling","title":"Message handling","text":"<p>In <code>on_message</code>, the server routes topics to <code>AgentControlHandler</code>:</p> <ul> <li><code>nadine/agent/control/look_at</code></li> <li>Payload: JSON <code>{\"x\": float, \"y\": float, \"z\": float}</code>.</li> <li> <p>Action: <code>agentcontrol_handler.lookAtPosition(...)</code> \u2192 <code>NadineControl.look_at_position(...)</code>.</p> </li> <li> <p><code>nadine/agent/control/speak</code></p> </li> <li>Payload: plain text string.</li> <li> <p>Action: <code>agentcontrol_handler.speak(text, volume=0)</code> \u2192 <code>NadineControl.make_nadine_speak(...)</code>.</p> </li> <li> <p><code>nadine/agent/control/animation</code></p> </li> <li>Payload: string/enum for a predefined animation.</li> <li>Action: forwarded to animation handling (e.g. <code>playAnimation</code> / <code>touchTarget</code> depending on implementation).</li> </ul>"},{"location":"control-runtime/#feedback-topics","title":"Feedback topics","text":"<p><code>NadineServer</code> also exposes:</p> <ul> <li><code>speakBegin()</code> \u2192 publishes <code>nadine/agent/feedback/start_speak</code> </li> <li><code>speakEnd()</code> \u2192 publishes <code>nadine/agent/feedback/end_speak</code></li> </ul> <p>These are called from <code>NadineControl</code> at the start and end of speaking to synchronize with the interaction layer and UI.</p>"},{"location":"control-runtime/#high-level-control-agentcontrolhandler","title":"High-Level Control (<code>AgentControlHandler</code>)","text":"<p><code>AgentControlHandler</code> wraps <code>NadineControl</code> to present a simple API to the rest of the system.</p>"},{"location":"control-runtime/#initialization","title":"Initialization","text":"<p>In <code>__init__(animation_xml_path)</code>:</p> <ul> <li>Creates <code>self.robot = NadineControl()</code>.  </li> <li>Calls <code>self.robot.load_animation_library(animation_xml_path)</code>.  </li> <li>Calls <code>self.robot.init_me()</code>:</li> <li>Initializes joints and default posture.</li> <li>Starts idle movement (blinking) threads.</li> <li>Sets up Azure TTS and checker.</li> </ul>"},{"location":"control-runtime/#key-methods","title":"Key methods","text":"<ul> <li><code>lookAtPosition(position)</code></li> <li>Expects a dict with <code>{\"x\", \"y\", \"z\"}</code>.  </li> <li> <p>Calls <code>self.robot.look_at_position(...)</code> to orient head/eyes.</p> </li> <li> <p><code>lookAtTarget(target)</code> / <code>endLookAt()</code></p> </li> <li> <p>Starts/stops continuous look\u2011at behavior for a named target.</p> </li> <li> <p><code>speak(phrase, volume)</code></p> </li> <li>Calls <code>self.robot.make_nadine_speak(phrase, volume)</code>.  </li> <li> <p><code>NadineControl</code> then:</p> <ul> <li>Ensures a suitable posture.</li> <li>Uses Azure TTS to generate audio and lip/jaw trajectories.</li> <li>Updates mouth\u2011related joints and triggers feedback via <code>nadine_server.speakBegin()/speakEnd()</code>.</li> </ul> </li> <li> <p><code>playAnimation(animation)</code></p> </li> <li>Maps a logical animation identifier (e.g. <code>WHY</code>, <code>WAVE_HAND</code>, <code>NOD_YES</code>) to an XML animation name.</li> <li>Calls <code>self.robot.play_animation(\"SomeAnimationName\")</code>.</li> </ul> <p>Other methods (point/greet/move) are stubs or thin wrappers and can be extended as needed.</p>"},{"location":"control-runtime/#core-robot-controller-nadinecontrol","title":"Core Robot Controller (<code>NadineControl</code>)","text":"<p><code>NadineControl</code> is responsible for turning high\u2011level requests into joint\u2011level trajectories and serial commands.</p>"},{"location":"control-runtime/#initialization-init_me","title":"Initialization (<code>init_me</code>)","text":"<p>Typical steps:</p> <ul> <li>Create 28 <code>Joint</code> objects (for Nadine\u2019s servos) with default positions.  </li> <li>Initialize the body configuration and idle state.  </li> <li>Create a <code>Checker</code> instance (drives playback and safety checks).  </li> <li>Create <code>AzureTTS</code> bound to the checker.  </li> <li>Start:</li> <li>A blink/idle movement thread (<code>endless_movements</code>).  </li> <li>A console input thread (<code>ask_for_text</code>) for debugging.  </li> <li>A look\u2011at target thread to maintain gaze when a target is set.</li> </ul>"},{"location":"control-runtime/#look-at-behavior","title":"Look-at behavior","text":"<p>Methods like <code>look_at_position(...)</code>, <code>look_at_target(...)</code>, and <code>end_look_at()</code>:</p> <ul> <li>Compute joint trajectories for head/eye servos based on a 3D position or named target.  </li> <li>Update associated <code>Joint</code> trajectories so that the checker loop sends appropriate serial commands.</li> </ul>"},{"location":"control-runtime/#speech-and-lip-sync-make_nadine_speak","title":"Speech and lip-sync (<code>make_nadine_speak</code>)","text":"<p>When <code>make_nadine_speak(text, volume)</code> is called:</p> <ul> <li>Ensure Nadine is in a speaking posture (e.g. <code>\"LOOKUPPostureDefault\"</code>).  </li> <li>Log the speech request.  </li> <li>Use <code>AzureTTS</code> to:</li> <li>Generate speech audio.</li> <li>Compute lip/jaw trajectories (<code>channel5</code>, <code>channel6</code>, <code>channel7</code>, etc.).  </li> <li>Clear and repopulate mouth-related joints with the new trajectories.  </li> <li>Set flags indicating that speech is in progress and call:</li> <li><code>nadine_server.speakBegin()</code> at start.  </li> <li><code>nadine_server.speakEnd()</code> when speech completes.</li> </ul>"},{"location":"control-runtime/#animation-execution","title":"Animation execution","text":"<p>For named animations:</p> <ul> <li><code>NadineControl</code> uses <code>AnimationLibrary</code> to retrieve per\u2011joint sequences for the given animation.  </li> <li>Populates each relevant <code>Joint</code>\u2019s trajectory.  </li> <li>A background loop (inside <code>Checker</code>) steps through frames, sending servo positions over <code>SerialComm</code>.</li> </ul>"},{"location":"control-runtime/#mqtt-topics-control-summary","title":"MQTT Topics (Control Summary)","text":"<p>Subscribed (Control consumes)</p> <ul> <li><code>nadine/agent/control/speak</code> </li> <li>Payload: <code>\"&lt;text to speak&gt;\"</code> </li> <li> <p>Effect: triggers TTS + lip\u2011sync and potentially an associated animation.</p> </li> <li> <p><code>nadine/agent/control/look_at</code> </p> </li> <li>Payload: <code>{\"x\": float, \"y\": float, \"z\": float}</code> </li> <li> <p>Effect: orients Nadine\u2019s head/eyes to look at the given 3D position.</p> </li> <li> <p><code>nadine/agent/control/animation</code> </p> </li> <li>Payload: string/enum identifier  </li> <li>Effect: plays a predefined gesture/posture animation.</li> </ul> <p>Published (Control produces)</p> <ul> <li><code>nadine/agent/feedback/start_speak</code> </li> <li> <p>Emitted when Nadine starts speaking.</p> </li> <li> <p><code>nadine/agent/feedback/end_speak</code> </p> </li> <li>Emitted when Nadine finishes speaking.</li> </ul> <p>These topics close the loop between perception (gaze), interaction (dialogue), and control (motion and speech).</p>"},{"location":"control-runtime/#extension-points","title":"Extension Points","text":"<p>Common places to extend the control layer:</p> <ul> <li>Add new animations</li> <li>Create a new XML file in <code>XMLAnimations/</code>.  </li> <li> <p>Add a case in <code>AgentControlHandler.playAnimation(...)</code> mapping an enum/name to your XML animation.</p> </li> <li> <p>Change idle behavior or default posture</p> </li> <li> <p>Modify <code>NadineControl.init_me()</code> and/or <code>endless_movements</code>.</p> </li> <li> <p>Swap TTS provider</p> </li> <li> <p>Implement a new TTS class and replace <code>AzureTTS</code> usage in <code>NadineControl</code>, making sure lip animation data still drives the same joints.</p> </li> <li> <p>Adapt to different hardware</p> </li> <li>Update <code>SerialComm</code> and <code>Checker</code> configurations to match your controller\u2019s serial port and protocol.</li> </ul>"},{"location":"interaction-agents/","title":"Interaction Layer \u2013 Agents &amp; Graph","text":"<p>This page documents the multi-agent graph that powers Nadine\u2019s dialogue, how each agent behaves, and how they are wired together using LangGraph.</p>"},{"location":"interaction-agents/#overview-of-the-agent-graph-graphpy","title":"Overview of the Agent Graph (<code>graph.py</code>)","text":"<p>The core of the interaction layer is a LangGraph <code>StateGraph</code> defined in <code>nadine/agents/graph.py</code>.</p> <p>Key components:</p> <ul> <li>CustomState (<code>state_schema.py</code>):</li> <li> <p>Holds:</p> <ul> <li><code>messages</code> (chat history)</li> <li><code>user_info</code> (ID, name, profile)</li> <li><code>conversation_memory</code>, <code>episode_memory</code></li> <li><code>visual_memory</code></li> <li><code>search_results</code>, <code>vision_results</code>, <code>knowledge_retrieval</code></li> <li><code>intent</code>, <code>affect</code>, <code>plan_steps</code>, <code>language</code></li> <li><code>name_confirmation</code>, <code>name_checked</code></li> </ul> </li> <li> <p>Top-level nodes:</p> </li> <li><code>intention_classifier</code></li> <li><code>memory_update_agent</code> / <code>memory_retrieve_agent</code> (ChromaDB\u2011backed textual memory + CLIP\u2011backed visual memory)</li> <li><code>affective_appraisal</code> / <code>affective_update</code></li> <li><code>orchestrator</code></li> <li>Tool agents: <code>search_agent</code>, <code>vision_agent</code>, <code>knowledge_rag_agent</code></li> <li> <p><code>response_agent</code></p> </li> <li> <p>Execution entry:</p> </li> <li><code>build_agent_graph()</code> returns a compiled workflow used by <code>DialogueManager</code>.</li> </ul>"},{"location":"interaction-agents/#intent-classifier","title":"Intent Classifier","text":"<p>Function: <code>classify_intent(state: CustomState) -&gt; CustomState</code></p> <p>Responsibilities:</p> <ul> <li>Reads the latest user message from <code>state[\"messages\"][-1].content</code>.  </li> <li>Uses a dedicated LLM (via <code>load_agent_llm(\"intention_classifier\")</code>) to classify the message into one of:</li> <li><code>first_greeting</code></li> <li><code>update_user_info</code></li> <li><code>end_conversation</code></li> <li><code>language_change</code></li> <li><code>continue_conversation</code></li> </ul> <p>Special behavior:</p> <ul> <li>If <code>language_change</code> is detected, tries to infer the requested language from the message content and updates <code>state[\"language\"]</code> accordingly, then collapses intent back to <code>continue_conversation</code>.</li> <li>If <code>state[\"name_confirmation\"]</code> contains <code>similar_names</code> and <code>name_checked</code> is <code>False</code>, forces <code>intent = \"update_user_info\"</code> so memory update logic can resolve the name.</li> </ul> <p>Graph wiring:</p> <ul> <li>From <code>START</code> \u2192 <code>intention_classifier</code>.  </li> <li>Conditional edges:</li> <li>To <code>memory_update_agent</code> if <code>intent == \"update_user_info\"</code>.</li> <li>Otherwise to <code>memory_retrieve_agent</code>.</li> </ul>"},{"location":"interaction-agents/#memory-retrieval-update-agents","title":"Memory Retrieval &amp; Update Agents","text":""},{"location":"interaction-agents/#memory-retrieval-memory_retrieve_agent","title":"Memory Retrieval (<code>memory_retrieve_agent</code>)","text":"<p>Function: <code>retrieve_memory(state: CustomState) -&gt; CustomState</code></p> <ul> <li>If memory agents are disabled (<code>NADINE_ENABLE_MEMORY_AGENTS=0</code>), logs and returns immediately.  </li> <li>Otherwise:</li> <li>Calls <code>get_user_specific_memory(state)</code> from <code>memory_retrieval_agent.py</code>:<ul> <li>Loads user profile from disk (<code>user_info.json</code>).</li> <li>Queries Chroma DB for:</li> <li>Best matching episode memory (events).</li> <li>Best matching conversation memory (past exchanges).</li> <li>Queries visual memory (memorable scenes) using CLIP similarity.</li> </ul> </li> <li>If name confirmation is needed, places details into <code>state[\"name_confirmation\"]</code>.  </li> <li>Otherwise, updates:<ul> <li><code>state[\"user_info\"]</code></li> <li><code>state[\"conversation_memory\"]</code></li> <li><code>state[\"episode_memory\"]</code></li> <li><code>state[\"visual_memory\"]</code></li> </ul> </li> </ul> <p>Graph wiring:</p> <ul> <li>From <code>intention_classifier</code> or <code>memory_update_agent</code> (depending on conditions).  </li> <li>Next:</li> <li><code>affective_appraisal</code> \u2192 <code>orchestrator</code>.</li> </ul>"},{"location":"interaction-agents/#memory-update-memory_update_agent","title":"Memory Update (<code>memory_update_agent</code>)","text":"<p>Function: <code>update_memory(state: CustomState) -&gt; CustomState</code> Wrapper around <code>memory_update_agent(state)</code> in <code>memory_update_agent.py</code>.</p> <p>Behaviors:</p> <ul> <li>If memory agents are disabled, returns immediately.  </li> <li>Skips update if name confirmation is still pending.  </li> <li>Builds a <code>mem_state</code> with:</li> <li><code>messages</code>, <code>user_info</code>, <code>intent</code>, <code>name_checked</code>.</li> <li>Calls <code>memory_update_agent(mem_state)</code> to:</li> <li>Extract user profile fields from conversation (name, company, location, hobbies, interests).</li> <li>Save/update <code>user_info.json</code> and <code>user_ids.json</code>.</li> <li>Optionally summarize and save episodic memory to Chroma.</li> <li>Merges results back into <code>state</code>:</li> <li><code>user_info</code>, <code>name_checked</code>, <code>name_confirmation</code>, <code>episodic_memory</code> flags, etc.</li> <li>If <code>name_checked</code> and <code>user_info</code> are valid:</li> <li>Sends name and ID to perception via:<ul> <li><code>mqtt_comm.send_user_info_to_face_recognition(user_name, user_id)</code></li> </ul> </li> </ul> <p>Graph wiring:</p> <ul> <li>Conditional edge from <code>intention_classifier</code>: <code>update_user_info</code> \u2192 <code>memory_update_agent</code>.  </li> <li>From <code>memory_update_agent</code>:</li> <li>If <code>name_confirmation</code> present \u2192 <code>response_agent</code> (ask the user to confirm).  </li> <li>If <code>intent == \"end_conversation\"</code> \u2192 <code>END</code>.  </li> <li>Else \u2192 <code>memory_retrieve_agent</code> (normal flow).</li> </ul>"},{"location":"interaction-agents/#name-confirmation-flow-high-level","title":"Name confirmation flow (high-level)","text":"<p>The name confirmation logic is split between the memory update agent and the DialogueManager:</p> <ol> <li>User shares their name (e.g., \u201cMy name is Alex\u201d):  </li> <li>The intent classifier sets <code>intent = \"update_user_info\"</code>.  </li> <li><code>memory_update_agent</code> extracts <code>user_name=\"Alex\"</code> and compares it against names in <code>user_ids.json</code> using fuzzy matching.</li> <li>Three possible outcomes from <code>save_user_info</code>:</li> <li>Strong match (score \u2265 <code>HIGH_SIMILARITY_THRESHOLD</code>):  <ul> <li>Immediately link to the existing profile for that user ID.  </li> <li>Return updated <code>user_info</code> and <code>name_checked=True</code> with no <code>name_confirmation</code>.</li> </ul> </li> <li>Ambiguous match (between <code>LOW_SIMILARITY_THRESHOLD</code> and <code>HIGH_SIMILARITY_THRESHOLD</code>):  <ul> <li>Return a <code>name_confirmation</code> payload with:</li> <li><code>given_name</code>, <code>similar_names</code>, <code>similar_ids</code>, <code>similarity_scores</code>.  </li> <li>The graph routes to <code>response_agent</code>, which asks the user to confirm the top suggested name (e.g., \u201cIs your name Alice?\u201d).</li> </ul> </li> <li>No match (below <code>LOW_SIMILARITY_THRESHOLD</code>):  <ul> <li>Proceed as a new user; create/update a profile with <code>user_name</code> and mark <code>name_checked=True</code>.</li> </ul> </li> <li>User answers the confirmation question:  </li> <li>On the next turn, <code>DialogueManager.name_confirmation(user_input)</code> inspects <code>state[\"name_confirmation\"]</code> and the user\u2019s reply:<ul> <li>If the user says \u201cyes / correct / that\u2019s me\u201d, the top candidate user ID is accepted and the full stored profile is loaded.  </li> <li>If the user clearly states a different name (e.g. \u201cNo, I\u2019m Alice Smith\u201d), a new profile is created/updated for that name.  </li> <li>If only one candidate remains and the user doesn\u2019t explicitly confirm, the <code>given_name</code> may be taken as the new canonical name.  </li> <li>Otherwise, the candidate list is rotated (pop the first suggestion) and another confirmation question will be asked later.</li> </ul> </li> <li>In every resolved case:<ul> <li><code>name_checked=True</code>, <code>name_confirmation={}</code>.  </li> <li><code>send_user_info_to_face_recognition(user_name, user_id)</code> is called, so perception stores/updates face embeddings under the correct ID.</li> </ul> </li> </ol> <p></p> <p>This loop ensures that:</p> <ul> <li>Faces, memories, and conversations are consistently linked to the correct user ID.  </li> <li>Ambiguous matches never silently overwrite existing users; they\u2019re always confirmed explicitly in dialogue.</li> </ul>"},{"location":"interaction-agents/#affective-system","title":"Affective System","text":""},{"location":"interaction-agents/#affective-appraisal","title":"Affective Appraisal","text":"<p>Function: <code>affective_appraisal(state: CustomState) -&gt; CustomState</code></p> <ul> <li>Ensures <code>state[\"affect\"]</code> has subkeys:</li> <li><code>personality</code>, <code>emotion</code>, <code>mood</code>, <code>params</code>.</li> <li>Uses <code>appraise_event_with_llm(...)</code> from <code>affective_system.py</code> to:</li> <li>Take the latest user message and contextual info (e.g. retrieved episode memories).  </li> <li>Produce a new emotion label/intensity.</li> <li>Calls <code>update_affect_state(...)</code> to update PAD mood/emotion state.</li> </ul> <p>Graph wiring:</p> <ul> <li>From <code>memory_retrieve_agent</code> \u2192 <code>affective_appraisal</code> \u2192 <code>orchestrator</code>.</li> </ul>"},{"location":"interaction-agents/#affective-update","title":"Affective Update","text":"<p>Function: <code>affective_update(state: CustomState) -&gt; CustomState</code></p> <ul> <li>Post-response hook (after <code>response_agent</code>).  </li> <li>Currently a pass-through, but is a good place to:</li> <li>Increment turn counters,  </li> <li>Adjust mood based on interaction outcome, etc.</li> </ul> <p>Graph wiring:</p> <ul> <li>From <code>response_agent</code> \u2192 <code>affective_update</code>.  </li> <li>Conditional edge:</li> <li>If <code>intent == \"end_conversation\"</code> \u2192 <code>memory_update_agent</code> (for final episodic save).  </li> <li>Else \u2192 <code>END</code>.</li> </ul>"},{"location":"interaction-agents/#orchestrator-sub-agents","title":"Orchestrator &amp; Sub-Agents","text":""},{"location":"interaction-agents/#contextualizer","title":"Contextualizer","text":"<p>Function: <code>contextualizer()</code> (used within <code>orchestrate</code>)</p> <ul> <li>Purpose: Summarizes conversation history to provide context to the orchestrator without passing the full chat history (which can cause invalid JSON output with long conversations).</li> <li>Location: <code>nadine/agents/context_summarizer.py</code></li> <li>Behavior:</li> <li>Takes chat history and the latest user message.</li> <li>Generates a concise summary/contextualized question that includes relevant context from the conversation.</li> <li>Returns a standalone question that can be understood without the full history.</li> <li>If no history exists or the question is standalone, returns the original question unchanged.</li> <li>Usage: Called automatically by the orchestrator when chat history exists.</li> </ul>"},{"location":"interaction-agents/#orchestrator-orchestrate","title":"Orchestrator (<code>orchestrate</code>)","text":"<p>Function: <code>orchestrate(state: CustomState) -&gt; CustomState</code></p> <ul> <li>Ensures <code>state[\"plan_steps\"]</code> is a list.  </li> <li>If a plan already exists, returns immediately.  </li> <li>Otherwise:</li> <li>Context Generation: If chat history exists, calls <code>contextualizer()</code> to generate conversation context (prevents invalid JSON from long histories).</li> <li>Builds an <code>orchestration_agent()</code> chain from <code>orchestration_agent.py</code>.  </li> <li>Passes:<ul> <li>Latest user message.</li> <li>Generated context (if available) - a summarized version of the conversation history.</li> </ul> </li> <li>Receives a JSON \"plan\" describing a sequence of steps, e.g.:<ul> <li><code>search_agent</code>, <code>knowledge_rag_agent</code>, <code>vision_agent</code>, <code>response_agent</code>.</li> </ul> </li> <li>Stores the plan in <code>state[\"plan_steps\"]</code>.</li> </ul> <p>Note: The orchestrator no longer receives the full <code>chat_history</code> directly to avoid JSON parsing errors with long conversations. Instead, it receives a summarized context from the contextualizer.</p> <p>Graph wiring:</p> <ul> <li>From <code>affective_appraisal</code> \u2192 <code>orchestrator</code>.  </li> <li>Conditional edges:</li> <li>If <code>plan_steps</code> empty \u2192 <code>response_agent</code>.  </li> <li>Else \u2192 first planned agent, e.g. <code>search_agent</code>, <code>vision_agent</code>, or <code>knowledge_rag_agent</code>.</li> </ul>"},{"location":"interaction-agents/#search-agent-get_search_results","title":"Search Agent (<code>get_search_results</code>)","text":"<p>Function: <code>get_search_results(state: CustomState) -&gt; CustomState</code></p> <ul> <li>Uses <code>search_agent()</code> to run a question-answering pipeline (web or external search).  </li> <li>Takes <code>state[\"plan_steps\"][0][\"message\"]</code> as the query.  </li> <li>Stores the result in <code>state[\"search_results\"]</code>.  </li> <li>Pops the executed step from <code>plan_steps</code>.</li> </ul> <p>Graph wiring:</p> <ul> <li>From <code>orchestrator</code> \u2192 <code>search_agent</code>.  </li> <li>Conditional edge:</li> <li>If <code>plan_steps</code> is now empty \u2192 <code>response_agent</code>.  </li> <li>Else \u2192 <code>orchestrator</code> (to schedule remaining steps).</li> </ul>"},{"location":"interaction-agents/#vision-agent-get_vision_results","title":"Vision Agent (<code>get_vision_results</code>)","text":"<p>Function: <code>get_vision_results(state: CustomState) -&gt; CustomState</code></p> <ul> <li>If vision agents are disabled (<code>NADINE_ENABLE_VISION_AGENT=0</code>):</li> <li>Logs and sets <code>state[\"vision_results\"]</code> to a fallback string.  </li> <li>Otherwise:</li> <li>Calls <code>vision_agent()</code> with the user question from <code>plan_steps</code>.  </li> <li>Stores the result in <code>state[\"vision_results\"]</code>.  </li> <li>Pops the executed step from <code>plan_steps</code>.</li> </ul> <p>Graph wiring:</p> <ul> <li>From <code>orchestrator</code> \u2192 <code>vision_agent</code>.  </li> <li>Conditional edge:</li> <li>If <code>plan_steps</code> empty \u2192 <code>response_agent</code>.  </li> <li>Else \u2192 <code>orchestrator</code>.</li> </ul>"},{"location":"interaction-agents/#knowledge-rag-agent-get_knowledge_retrieval","title":"Knowledge RAG Agent (<code>get_knowledge_retrieval</code>)","text":"<p>Function: <code>get_knowledge_retrieval(state: CustomState) -&gt; CustomState</code></p> <ul> <li>Calls <code>get_related_knowledge(state)</code> from <code>knowledge_RAG_agent.py</code>.  </li> <li>This function:</li> <li>Builds/loads a Chroma vectorstore from <code>interaction/db/knowledge/rag_files/</code>.  </li> <li>Uses <code>OllamaEmbeddings</code> to embed knowledge sections.  </li> <li>Retrieves top\u2011k relevant chunks for the current question.  </li> <li>Optionally displays related visuals in the UI.  </li> <li>Stores the retrieved text in <code>state[\"knowledge_retrieval\"]</code>.  </li> <li>Pops the executed step from <code>plan_steps</code>.</li> </ul> <p>Graph wiring:</p> <ul> <li>From <code>orchestrator</code> \u2192 <code>knowledge_rag_agent</code>.  </li> <li>Conditional edge:</li> <li>If <code>plan_steps</code> empty \u2192 <code>response_agent</code>.  </li> <li>Else \u2192 <code>orchestrator</code>.</li> </ul>"},{"location":"interaction-agents/#response-agent","title":"Response Agent","text":"<p>Function: <code>get_final_response(state: CustomState) -&gt; CustomState</code></p> <p>Wrapper around <code>response_agent()</code> from <code>response_agent.py</code>:</p> <ul> <li>If <code>state[\"name_confirmation\"]</code> contains <code>similar_names</code>, returns early to trigger a name-confirmation prompt.  </li> <li>Otherwise:</li> <li>Clears <code>plan_steps</code>.  </li> <li>Calls <code>response_agent()(state)</code>:<ul> <li>Builds a composite user packet with:</li> <li>Search, vision, knowledge results.</li> <li>Current time/date.</li> <li>Affective state (emotion + mood text).</li> <li>Optional visual memory image (encoded as a data URL).</li> <li>Sends a system prompt describing Nadine\u2019s persona and language rules.</li> <li>Invokes the configured <code>response_llm</code>.</li> <li>Returns the final message text.</li> </ul> </li> <li>Writes the response into <code>state[\"final_message\"]</code>.  </li> <li>Resets:<ul> <li><code>conversation_memory</code>, <code>episode_memory</code>,</li> <li><code>search_results</code>, <code>vision_results</code>, <code>knowledge_retrieval</code>.</li> </ul> </li> </ul> <p>Graph wiring:</p> <ul> <li>Called from many points:</li> <li>Directly after <code>orchestrator</code> if no tools are needed.  </li> <li>After tool agents if <code>plan_steps</code> is empty.  </li> <li>From <code>memory_update_agent</code> when name confirmation is required.  </li> <li>After <code>response_agent</code>, the graph always goes to <code>affective_update</code>.</li> </ul>"},{"location":"interaction-agents/#testing-test-harnesses","title":"Testing &amp; Test Harnesses","text":"<p>All agents include comprehensive test harnesses that can be run directly:</p> <ul> <li>Intent Classifier: <code>python intention_classifier_test.py</code> - Tests intent classification with various message types.</li> <li>Memory Update Agent: <code>python memory_update_agent.py</code> - Tests user info extraction and episodic memory summarization.</li> <li>Orchestration Agent: <code>python orchestration_agent.py</code> - Tests routing to different sub-agents.</li> <li>Search Agent: <code>python search_agent.py</code> - Tests search router and answer summarization.</li> <li>Vision Agent: <code>python vision_agent.py</code> - Tests vision router and image description (with real images from <code>db/nadine_view/samples/</code>).</li> <li>Response Agent: <code>python response_agent.py</code> - Tests response generation with various affect states and contexts.</li> <li>Contextualizer: <code>python context_summarizer.py</code> - Tests conversation context summarization.</li> </ul> <p>Centralized Test Samples: All test cases are stored in <code>agent_test_samples.py</code> for easy maintenance and reuse: - <code>INTENT_CLASSIFICATION_TEST_MESSAGES</code> - <code>USER_INFO_TEST_MESSAGES</code> - <code>EPISODIC_MEMORY_TEST_CONVERSATIONS</code> - <code>ORCHESTRATION_TEST_CASES</code> - <code>SEARCH_ROUTER_TEST_CASES</code> - <code>SEARCH_ANSWER_TEST_CASES</code> - <code>VISION_ROUTER_TEST_CASES</code> - <code>VISION_DESCRIPTION_TEST_CASES</code> - <code>VISION_REAL_IMAGE_QUESTIONS</code> - <code>RESPONSE_AGENT_TEST_CASES</code> - <code>CONTEXTUALIZER_TEST_CASES</code></p>"},{"location":"interaction-agents/#putting-it-all-together","title":"Putting It All Together","text":"<p>For each user input, the graph runs roughly:</p> <ol> <li><code>intention_classifier</code> \u2192 classify the message.  </li> <li><code>memory_update_agent</code> (for profile updates) or <code>memory_retrieve_agent</code> (for context recall).  </li> <li><code>affective_appraisal</code> \u2192 update emotion/mood.  </li> <li><code>orchestrator</code> \u2192 decide which sub-agents (if any) to call:</li> <li>Contextualizer (if history exists) \u2192 generates conversation context.</li> <li>Orchestrator uses context + user message to route requests.</li> <li><code>search_agent</code> / <code>vision_agent</code> / <code>knowledge_rag_agent</code> (optional).  </li> <li><code>response_agent</code> \u2192 final reply.  </li> <li><code>affective_update</code> \u2192 finalize affect and optionally trigger further memory updates.</li> </ol> <p>Detailed memory mechanics are documented on the Interaction Layer / Memory &amp; RAG page.</p>"},{"location":"interaction-memory-rag/","title":"Interaction Layer \u2013 Memory &amp; RAG","text":"<p>This page details how Nadine\u2019s interaction layer manages user memory (profiles, episodic memory, visual memory) and knowledge retrieval (RAG) on top of ChromaDB and CLIP-based embeddings.</p>"},{"location":"interaction-memory-rag/#user-profiles-user_infojson-user_idsjson","title":"User Profiles (<code>user_info.json</code> &amp; <code>user_ids.json</code>)","text":"<p>User profile information is stored under:</p> <ul> <li><code>interaction/db/memory/user_profiles/&lt;user_id&gt;/user_info.json</code></li> </ul> <p>Each <code>user_info.json</code> contains fields like:</p> <ul> <li><code>user_id</code>, <code>user_name</code></li> <li><code>current_company</code>, <code>current_position</code>, <code>location</code></li> <li><code>hobbies</code>, <code>interests</code></li> <li><code>face_image_path</code></li> <li><code>last_updated</code></li> </ul> <p>The file:</p> <ul> <li><code>interaction/db/user_ids.json</code></li> </ul> <p>Maps:</p> <ul> <li><code>user_id</code> \u2192 <code>user_name</code></li> </ul> <p>This mapping is used to quickly resolve names, detect similar names, and coordinate between interaction and perception layers.</p> <p>Key helpers:</p> <ul> <li><code>user_info_init(user_id, user_name)</code> \u2013 create a default profile.  </li> <li><code>fetch_user_info(user_id, user_name)</code> \u2013 load or initialize profile on disk.  </li> <li><code>update_user_ids(user_id, user_name)</code> \u2013 update <code>user_ids.json</code> after name changes.</li> </ul>"},{"location":"interaction-memory-rag/#memory-update-agent-memory_update_agentpy","title":"Memory Update Agent (<code>memory_update_agent.py</code>)","text":"<p>The memory update agent has two main responsibilities:</p> <ol> <li>Structured user profile updates based on conversation (e.g., \u201cMy name is Alice, I work at Trafigura in Geneva\u201d).  </li> <li>Episodic memory storage that summarizes important interaction episodes.</li> </ol>"},{"location":"interaction-memory-rag/#structured-user-profile-updates","title":"Structured user profile updates","text":"<p>When <code>state[\"intent\"] == \"update_user_info\"</code>:</p> <ul> <li>A dedicated LLM (<code>load_agent_llm(\"memory_update_agent\")</code>) is prompted to extract structured fields:</li> <li><code>user_name</code></li> <li><code>current_company</code></li> <li><code>current_position</code></li> <li><code>location</code></li> <li><code>hobbies</code></li> <li><code>interests</code></li> <li>The output is parsed into a <code>MemoryUpdate</code> Pydantic model.</li> <li><code>save_user_info(...)</code>:</li> <li>Loads existing <code>user_info.json</code> (or initializes one).</li> <li>Merges the new fields (deduplicating hobbies/interests).</li> <li>Updates <code>last_updated</code>.</li> <li>Ensures <code>user_ids.json</code> is consistent.</li> <li>Handles name similarity and confirmation:<ul> <li>Computes a similarity score between <code>user_name</code> and all names in <code>user_ids.json</code> using Levenshtein ratio.</li> <li>If one name is a strong match (<code>HIGH_SIMILARITY_THRESHOLD</code>):</li> <li>Immediately links to that existing user and returns the stored profile.</li> <li>If several names are moderately similar (<code>LOW_SIMILARITY_THRESHOLD</code>):</li> <li>Returns a <code>name_confirmation</code> structure containing:<ul> <li><code>given_name</code>, <code>similar_names</code>, <code>similar_ids</code>, <code>similarity_scores</code>.</li> </ul> </li> <li>The graph then routes to <code>response_agent</code>, which asks the user to confirm the top candidate.</li> <li>If no reasonable match is found:</li> <li>Proceeds as a new user; <code>name_checked</code> is set and a new profile is created/updated.</li> </ul> </li> </ul> <p>The <code>update_memory</code> wrapper in <code>graph.py</code> merges the returned <code>user_info</code>, <code>name_checked</code>, and <code>name_confirmation</code> fields back into the graph state. On the next user turn, the DialogueManager consumes this state and either accepts a suggested existing user, creates a new one, or rotates to the next suggestion, before finally sending the confirmed <code>user_name</code>/<code>user_id</code> pair back to perception for face-linking.</p>"},{"location":"interaction-memory-rag/#episodic-memory","title":"Episodic memory","text":"<p>When <code>intent != \"update_user_info\"</code> (e.g., end of a conversation):</p> <ul> <li>The agent summarizes the last part of the conversation into an episode:</li> <li>Builds a short transcript of recent <code>Human</code>/<code>AI</code> turns.</li> <li>LLM extracts <code>observation</code>, <code>thought</code>, <code>action</code>, <code>result</code> into an <code>EpisodicSave</code> model.</li> <li><code>save_episodic_memory(...)</code>:</li> <li>Skips save if <code>user_name</code> is missing or still <code>\"Unknown\"</code>.  </li> <li>Otherwise:<ul> <li>Builds <code>conversation_text</code> and <code>episode_text</code>.</li> <li>Creates two new documents in Chroma, tagged with:</li> <li><code>user_id</code></li> <li><code>timestamp</code></li> <li><code>memory_type</code> = <code>\"conversation\"</code> or <code>\"episode\"</code>.</li> <li>Stores them in a Chroma collection named after the <code>user_id</code>.</li> </ul> </li> </ul> <p>The result flag <code>saved</code> is merged back into the graph state as <code>episodic_memory</code> information.</p>"},{"location":"interaction-memory-rag/#memory-retrieval-memory_retrieval_agentpy","title":"Memory Retrieval (<code>memory_retrieval_agent.py</code>)","text":"<p>When the graph needs context about a user, <code>get_user_specific_memory(state)</code> is called.</p>"},{"location":"interaction-memory-rag/#textual-memory-chroma","title":"Textual memory (Chroma)","text":"<ul> <li>Finds the Chroma collection for the current <code>user_id</code>.  </li> <li>Runs two independent similarity queries based on the latest user message:</li> <li>One over documents where <code>memory_type == \"episode\"</code>.  </li> <li>One over documents where <code>memory_type == \"conversation\"</code>.  </li> <li>Returns the closest matching:</li> <li><code>episode_memory</code></li> <li><code>conversation_memory</code></li> </ul> <p>If no Chroma collection exists yet, the function degrades gracefully and returns <code>None</code> for these fields.</p>"},{"location":"interaction-memory-rag/#visual-memory","title":"Visual memory","text":"<p>The same function also calls <code>_retrieve_best_visual_memory(state, user_id)</code> to:</p> <ul> <li>Inspect memorable scenes stored by the perception layer under:</li> <li><code>interaction/db/memory/user_profiles/&lt;user_id&gt;/memorable_scenes/</code></li> <li>Use CLIP text embeddings of the current user question to:</li> <li>Compare against stored image embeddings (from perception).</li> <li>Optionally combine similarity with stored scene descriptions.</li> <li>Select the most relevant visual memory if its combined similarity exceeds a threshold:</li> <li>Controlled by:<ul> <li><code>interaction.visual_memory.similarity_threshold</code></li> <li><code>interaction.visual_memory.retrieval_alpha</code></li> </ul> </li> </ul> <p>The output is a minimal dict:</p> <ul> <li><code>{\"image_path\": ..., \"scene_id\": ..., \"similarity\": ...}</code> or <code>None</code>.</li> </ul> <p>This is placed in <code>state[\"visual_memory\"]</code> and later consumed by the response agent:</p> <ul> <li><code>response_agent</code> attaches the image as an <code>image_url</code> content item, so the response LLM can reason over both text and visuals.</li> </ul>"},{"location":"interaction-memory-rag/#knowledge-rag-agent-knowledge_rag_agentpy","title":"Knowledge RAG Agent (<code>knowledge_RAG_agent.py</code>)","text":"<p>Nadine\u2019s long-term system knowledge lives in:</p> <ul> <li><code>interaction/db/knowledge/rag_files/</code></li> </ul> <p>The knowledge RAG agent (<code>get_related_knowledge(state)</code>) works as follows:</p> <ol> <li>On first run:</li> <li>Loads all files from <code>rag_files/</code> using <code>TextLoader</code>.  </li> <li>Splits them into chunks grouped by markdown headers (<code>#</code>, <code>##</code>).  </li> <li> <p>Builds a Chroma vectorstore:</p> <ul> <li>Embeddings: <code>OllamaEmbeddings(model=\"nomic-embed-text\")</code>.  </li> <li>Collection name: <code>\"nadine-knowledge\"</code>.  </li> <li>Persist directory: <code>interaction/db/knowledge/chroma/</code>.</li> </ul> </li> <li> <p>On later runs:</p> </li> <li> <p>Reuses the existing Chroma collection if present.</p> </li> <li> <p>For each query:</p> </li> <li>Uses the LangGraph state\u2019s last message as the query text.  </li> <li>Retrieves the top 1 most relevant chunk.  </li> <li>Flattens the chunk into a single string (with markup removed).  </li> <li>Optionally:<ul> <li>Detects and displays a visual source (if indicated in the chunk metadata).  </li> </ul> </li> <li>Returns a formatted string summary.</li> </ol> <p>The result is stored in <code>state[\"knowledge_retrieval\"]</code> and is passed into the response agent as part of the user packet. For a deeper view of how this knowledge is combined with other agents, see Interaction Layer \u2013 Agents &amp; Graph.</p>"},{"location":"interaction-memory-rag/#visual-memory-configuration-interactionconfigyaml","title":"Visual Memory Configuration (<code>interaction/config.yaml</code>)","text":"<p>The visual memory retrieval behavior is configured via:</p> <pre><code>interaction:\n  visual_memory:\n    similarity_threshold: 0.6\n    retrieval_alpha: 0.7\n</code></pre> <ul> <li><code>similarity_threshold</code>:</li> <li>Minimum combined CLIP similarity (text\u2013image + text\u2013description) needed to use a visual memory.</li> <li><code>retrieval_alpha</code>:</li> <li>Blend factor between:<ul> <li><code>sim_image</code> (text vs. image embedding)</li> <li><code>sim_desc</code> (text vs. description embedding)</li> </ul> </li> <li>Effective similarity:<ul> <li><code>alpha * sim_image + (1 - alpha) * sim_desc</code></li> </ul> </li> </ul> <p>Adjust these values to make visual memory more or less prominent in responses.</p>"},{"location":"interaction-memory-rag/#how-everything-fits-together","title":"How Everything Fits Together","text":"<p>On each turn:</p> <ol> <li>Memory retrieval (<code>retrieve_memory</code>):</li> <li>Loads textual + visual memories for the current user.  </li> <li> <p>Passes them to the affective system and orchestrator.</p> </li> <li> <p>Tools &amp; reasoning (search, vision, knowledge RAG):</p> </li> <li> <p>Provide up-to-date external and internal knowledge.  </p> </li> <li> <p>Response agent:</p> </li> <li>Sees:<ul> <li>Conversation + episodic memory summaries.</li> <li>Relevant visual memory (if any).</li> <li>Knowledge chunks from RAG.</li> </ul> </li> <li> <p>Generates a response that is both contextual and personalized.</p> </li> <li> <p>Memory update (<code>update_memory</code>):</p> </li> <li>Incorporates new user profile data and episodic memories.  </li> <li>Keeps <code>user_info</code>, Chroma DB, and <code>user_ids.json</code> in sync over time.</li> </ol> <p>Together, these components make Nadine\u2019s interactions cumulative, personalized, and multimodal across sessions.</p>"},{"location":"interaction-overview/","title":"Interaction Layer \u2013 Overview","text":"<p>The interaction layer is the main brain of Nadine. It runs a multi-agent dialogue system (based on LangGraph) that:</p> <ul> <li>Listens to the user via speech-to-text (STT)</li> <li>Routes requests across specialized LLM agents (search, vision, RAG, memory, affect)</li> <li>Maintains user-specific memory and affective state</li> <li>Produces responses and sends them to the control layer via MQTT (speech + animation)</li> </ul> <p>If you are new to the project, read this page first, then see the Runtime &amp; MQTT and Agents &amp; Graph pages for deeper details.</p>"},{"location":"interaction-overview/#responsibilities","title":"Responsibilities","text":"<ul> <li>Conversation management</li> <li>Turn raw speech into clean text.</li> <li>Run a multi-agent graph (intent \u2192 memory \u2192 affect \u2192 tools \u2192 response).</li> <li> <p>Keep rolling chat history per user.</p> </li> <li> <p>User modeling &amp; memory</p> </li> <li>Track user ID, name, and profile data.</li> <li>Store and retrieve episodic and conversation memories.</li> <li> <p>Integrate visual memories (from the perception layer).</p> </li> <li> <p>Reasoning &amp; tools</p> </li> <li>Use web/knowledge search (search agent).</li> <li>Retrieve from Nadine\u2019s internal knowledge (knowledge RAG agent, backed by ChromaDB).</li> <li> <p>Perform visual reasoning (vision agent).</p> </li> <li> <p>Multi-modal output</p> </li> <li>Generate natural language responses with an LLM.</li> <li>Coordinate speech and robot animations via MQTT.</li> </ul>"},{"location":"interaction-overview/#directory-structure-interaction","title":"Directory Structure (Interaction)","text":"<p>Under <code>interaction/</code>:</p> <ul> <li><code>config.yaml</code></li> <li>LLM profiles (<code>small_llm</code>, <code>big_llm</code>, <code>response_llm</code>, <code>vision_llm</code>).</li> <li>Agent \u2192 LLM profile mapping.</li> <li> <p>Visual-memory retrieval parameters.</p> </li> <li> <p><code>run.sh</code></p> </li> <li>Activates the <code>nadine</code> conda env and runs <code>python3 -m nadine</code>.</li> <li> <p>Sets up a cleanup hook for Ollama models on exit.</p> </li> <li> <p><code>nadine/__main__.py</code></p> </li> <li> <p>Entry point for the interaction layer:</p> <ul> <li>Loads env variables from <code>interaction/.env</code>.</li> <li>Initializes:</li> <li><code>UI</code></li> <li><code>STTManager</code></li> <li><code>MQTTCommunication</code></li> <li><code>DialogueManager</code> (multi-agent graph wrapper)</li> <li>Provides:</li> <li>Normal mode: full voice + UI + MQTT.</li> <li><code>--nomqtt</code>: run without MQTT.</li> <li><code>--chatmode</code>: text-only REPL using <code>DialogueManager</code> directly.</li> </ul> </li> <li> <p><code>nadine/common/</code></p> </li> <li><code>loggers.py</code> \u2013 logging utilities (<code>LoggersFactory</code>).</li> <li><code>language.py</code> \u2013 language enum and helpers.</li> <li><code>translation.py</code>, <code>translation_llm.py</code> \u2013 text translation tools.</li> <li> <p><code>mqtt_comm.py</code> \u2013 interaction-layer MQTT client and helpers.</p> </li> <li> <p><code>nadine/stt/</code></p> </li> <li> <p><code>google_stt.py</code>, <code>stt.py</code> \u2013 Google Cloud Speech-to-Text integration and microphone handling.</p> </li> <li> <p><code>nadine/ui/</code></p> </li> <li> <p><code>ui.py</code> \u2013 Qt/GUI window for monitoring interactions (user text, agent reply, status).</p> </li> <li> <p><code>nadine/agents/</code></p> </li> <li>Multi-agent graph and specialized agents (documented in Agents &amp; Graph and Memory &amp; RAG pages).</li> </ul>"},{"location":"interaction-overview/#high-level-flow","title":"High-Level Flow","text":"<p>Normal (voice) mode:</p> <ol> <li>User speaks \u2192 <code>STTManager</code> converts audio to text.  </li> <li><code>Nadine.user_speech_detected</code>:</li> <li>Suspends STT while the agent is speaking.</li> <li>Translates non-English input into English (if needed).</li> <li>Calls <code>DialogueManager.processInput(text_en)</code>.</li> <li>DialogueManager:</li> <li>Appends the user message to chat history.</li> <li>Syncs state with any detected face-recognition user.</li> <li>Runs the LangGraph multi-agent workflow:<ul> <li>Intent classification</li> <li>Memory retrieval/update</li> <li>Affective appraisal</li> <li>Contextualizer (generates conversation context from history)</li> <li>Orchestrator (routes to sub-agents using context)</li> <li>Sub-agents (search, vision, knowledge RAG)</li> <li>Response generation</li> </ul> </li> <li>Produces a final response text (and affect state).</li> <li>UI is updated (user input + agent output).  </li> <li>MQTTCommunication.speak sends the English response (translated to target language if needed) over:</li> <li><code>nadine/agent/control/speak</code></li> <li>After speaking finishes, STT is re-activated to listen for the next utterance.</li> </ol> <p>In chat mode, steps are similar but:</p> <ul> <li>No STT or UI.  </li> <li>The loop is a simple input/print REPL calling <code>DialogueManager.processInput</code>.</li> </ul>"},{"location":"interaction-overview/#configuration-interactionconfigyaml","title":"Configuration (<code>interaction/config.yaml</code>)","text":"<p>The interaction config binds agents to LLM profiles and tunes visual memory:</p> <ul> <li><code>interaction.llm</code></li> <li><code>small_llm</code> \u2013 e.g., <code>granite4:350m</code> (low-cost tasks).</li> <li><code>big_llm</code> \u2013 e.g., <code>mistral-small3.2</code> for heavier reasoning.</li> <li><code>response_llm</code> \u2013 main conversation LLM (typically same as <code>big_llm</code>).</li> <li> <p><code>vision_llm</code> \u2013 dedicated profile for vision agent.</p> </li> <li> <p><code>interaction.agents</code></p> </li> <li> <p>Maps logical agents to one of the above profiles, e.g.:</p> <ul> <li><code>orchestration_agent</code> \u2192 <code>big_llm</code></li> <li><code>search_answer</code> \u2192 <code>small_llm</code></li> <li><code>response_agent</code> \u2192 <code>response_llm</code></li> </ul> </li> <li> <p><code>interaction.visual_memory</code></p> </li> <li><code>similarity_threshold</code> \u2013 CLIP similarity cut-off for using visual memory.  </li> <li><code>retrieval_alpha</code> \u2013 weight between image vs. description similarity.</li> </ul> <p>These settings are read mainly via <code>nadine.agents.utils.load_agent_llm</code> and the visual-memory helpers in <code>memory_retrieval_agent.py</code>.</p>"},{"location":"interaction-overview/#where-to-go-next","title":"Where to Go Next","text":"<ul> <li>Interaction Layer / Runtime &amp; MQTT \u2013 deep dive into the DialogueManager, STT/UI, and MQTT topics.  </li> <li>Interaction Layer / Agents &amp; Graph \u2013 detailed description of the LangGraph workflow and each agent.  </li> <li>Interaction Layer / Memory &amp; RAG \u2013 how user profiles, episodic memory, visual memory, and RAG work together.</li> </ul>"},{"location":"interaction-runtime/","title":"Interaction Layer \u2013 Runtime &amp; MQTT","text":"<p>This page explains how the interaction layer runs at runtime: from microphone input, through the dialogue manager and multi-agent graph, to MQTT messages that drive speech and animations.</p>"},{"location":"interaction-runtime/#runtime-entrypoint-nadine__main__py","title":"Runtime Entrypoint (<code>nadine/__main__.py</code>)","text":"<p><code>python -m nadine</code> launches the interaction stack.</p>"},{"location":"interaction-runtime/#initialization","title":"Initialization","text":"<p><code>Nadine.__init__(nomqtt: bool)</code>:</p> <ul> <li>Loads environment variables from <code>interaction/.env</code>.  </li> <li>Creates:</li> <li><code>logger</code> via <code>LoggersFactory.getLogger()</code>.</li> <li><code>Translation</code> for multi-language translations.</li> <li><code>UI</code> window (status, user text, agent reply).</li> <li><code>STTManager</code> for microphone + Google STT.</li> <li><code>MQTTCommunication</code> for all interaction-layer MQTT I/O.</li> <li><code>DialogueManager</code> for multi-agent conversation logic.</li> </ul>"},{"location":"interaction-runtime/#modes","title":"Modes","text":"<p><code>__main__.py</code> supports:</p> <ul> <li>Default mode: voice + UI + MQTT.</li> <li><code>nadine = Nadine(nomqtt=False)</code></li> <li><code>nadine.start_all()</code></li> <li><code>--nomqtt</code>:</li> <li>Runs without MQTT (e.g. for offline experimentation).</li> <li><code>--chatmode</code>:</li> <li>Text-only mode:<ul> <li>Creates <code>DialogueManager()</code> directly.</li> <li>Simple REPL in the console:</li> <li>User types a message, DM returns text.</li> </ul> </li> </ul>"},{"location":"interaction-runtime/#voice-interaction-flow","title":"Voice Interaction Flow","text":"<p>In normal mode:</p> <ol> <li><code>Nadine.start_all()</code>:</li> <li>Starts STT listening.</li> <li>Sets initial UI status (microphone, user availability, etc.).</li> <li> <p>Starts the UI event loop.</p> </li> <li> <p>User speaks:</p> </li> <li> <p><code>STTManager</code> converts audio to text and calls <code>Nadine.user_speech_detected(text)</code>.</p> </li> <li> <p>user_speech_detected:</p> </li> <li>Suspends STT while Nadine is speaking (when MQTT is enabled).</li> <li>Translates user input to English if current <code>language</code> is not English.</li> <li>Calls <code>DialogueManager.processInput(text_en)</code>.</li> <li>Updates the UI (user input + agent output).</li> <li> <p>Calls <code>mqtt_comm.speak(reply_en, self.language)</code> to trigger speech and animation.</p> </li> <li> <p>After response:</p> </li> <li>STT is re-activated.</li> <li>UI status is updated again to reflect current listening/speaking state.</li> </ol> <p>This loop repeats for each user utterance.</p>"},{"location":"interaction-runtime/#dialoguemanager-runtime-dmpy","title":"DialogueManager Runtime (<code>dm.py</code>)","text":"<p><code>DialogueManager.processInput(user_input: str)</code> orchestrates the main interaction logic.</p>"},{"location":"interaction-runtime/#initialization-__init__","title":"Initialization (<code>__init__</code>)","text":"<ul> <li>Creates:</li> <li><code>mqtt_comm</code> \u2013 shared <code>MQTTCommunication</code> instance.</li> <li><code>logger</code> \u2013 shared logger.</li> <li><code>chat_history</code> \u2013 list of LangChain <code>HumanMessage</code>/<code>AIMessage</code>.</li> <li><code>user_id</code> \u2013 unique ID via <code>generate_unique_user_id()</code>.</li> <li><code>user_info</code> \u2013 default profile dict via <code>user_info_init(user_id)</code>.</li> <li><code>c_state</code> \u2013 current graph state (custom state dict).</li> <li><code>multi_agent_graph</code> \u2013 compiled LangGraph from <code>build_agent_graph()</code>.</li> <li><code>conversation_limit</code> \u2013 how many recent turns to keep in <code>chat_history</code>.</li> <li>Calls <code>warmup_llms()</code> to pre-initialize key LLMs.</li> </ul>"},{"location":"interaction-runtime/#state-refresh-name-confirmation","title":"State refresh &amp; name confirmation","text":"<p>Before invoking the graph, DM:</p> <ul> <li>Checks face-recognition info via <code>mqtt_comm.get_detected_user_info()</code>.  </li> <li><code>_refresh_state(detected_user_id)</code>:</li> <li>If a different user was detected:<ul> <li>Loads their <code>user_info.json</code> from the interaction DB.</li> <li>Resets MQTT detection state.</li> </ul> </li> <li>Updates <code>c_state</code> via <code>default_custom_state(c_state, chat_history, user_info)</code>.</li> </ul> <p>If the memory agents previously requested name confirmation, <code>name_confirmation(user_input)</code>:</p> <ul> <li>Handles:</li> <li>Confirming a suggested existing user.  </li> <li>Creating a new user from a name explicitly mentioned by the user.  </li> <li>Rotating through remaining candidate names if needed.  </li> <li>Synchronizes user info back to face recognition via MQTT.</li> </ul>"},{"location":"interaction-runtime/#graph-invocation","title":"Graph invocation","text":"<p>After state prep:</p> <ul> <li><code>results = multi_agent_graph.invoke(self.c_state)</code></li> </ul> <p>The result includes:</p> <ul> <li>Updated <code>user_info</code></li> <li>Updated <code>affect</code> state</li> <li>Updated memories</li> <li><code>final_message</code> \u2013 raw response text/JSON</li> <li>Optional <code>name_confirmation</code> payload</li> <li><code>intent</code> \u2013 classified intent (e.g., <code>first_greeting</code>, <code>end_conversation</code>)</li> </ul> <p>The DM:</p> <ul> <li>Optionally calls <code>set_language_callback</code> if <code>results[\"language\"]</code> changed.  </li> <li>Adopts <code>self.c_state = results</code>.  </li> <li>Ensures <code>user_info</code> and <code>user_id</code> in DM align with graph output.  </li> <li>Extracts:</li> <li>Final text + emotion via <code>_extract_robot_response(results)</code>.  </li> <li>Updates <code>chat_history</code> with the new AI message.  </li> <li>Resets chat history on <code>end_conversation</code> and trims to <code>conversation_limit</code> messages.</li> </ul>"},{"location":"interaction-runtime/#motion-side-effects","title":"Motion side-effects","text":"<p>For certain intents (<code>first_greeting</code>, <code>end_conversation</code>), DM asks the control layer to wave:</p> <ul> <li><code>self.mqtt_comm.give_wave()</code> \u2192 publishes a <code>nadine/agent/control/animation</code> command.</li> </ul>"},{"location":"interaction-runtime/#mqtt-topics-interaction-perspective","title":"MQTT Topics (Interaction Perspective)","text":"<p>The interaction layer uses <code>MQTTCommunication</code> as its main MQTT client.</p> <p>Subscribed</p> <ul> <li><code>nadine/graph/user_detected</code></li> <li>From perception layer.</li> <li>Payload: <code>{\"user_name\": str, \"user_id\": str, \"confidence\": float}</code>.</li> <li> <p>Used to track which user is currently in front of Nadine.</p> </li> <li> <p><code>nadine/graph/face_stored</code></p> </li> <li>From perception layer.</li> <li>Payload: <code>{\"user_name\": str, \"user_id\": str, \"status\": \"face_stored\" | \"face_stored_new_user\"}</code>.</li> <li> <p>Used to update <code>interaction/db/user_ids.json</code> when new users are added.</p> </li> <li> <p><code>nadine/agent/feedback/start_speak</code>, <code>nadine/agent/feedback/end_speak</code></p> </li> <li>From control layer.</li> <li>Indicate when Nadine starts/finishes speaking.</li> </ul> <p>Published</p> <ul> <li><code>nadine/face_recognition/user_info</code></li> <li>To perception layer.</li> <li>Payload: <code>{\"user_name\": ..., \"user_id\": ...}</code>.</li> <li> <p>Used when memory agents or DM finalize a user\u2019s name \u2192 triggers face storage.</p> </li> <li> <p><code>nadine/perception/capture_current_view</code></p> </li> <li>To perception layer.</li> <li>Payload: image path string.</li> <li> <p>Used to request a snapshot for vision/visual memory.</p> </li> <li> <p><code>nadine/agent/control/speak</code></p> </li> <li>To control layer.</li> <li>Payload: final response text (in the appropriate language).</li> <li> <p>Triggers TTS + lip-sync + associated animations.</p> </li> <li> <p><code>nadine/agent/control/animation</code></p> </li> <li>To control layer.</li> <li>Payload: animation name.</li> <li>Used by helper methods like <code>give_wave</code>, <code>give_greeting</code>, <code>give_smile</code>, etc.</li> </ul>"},{"location":"interaction-runtime/#quick-dev-tips","title":"Quick Dev Tips","text":"<ul> <li>To debug the LangGraph flow in isolation, run <code>graph.py</code> directly (it has a CLI <code>main()</code> loop).  </li> <li>To test the dialogue manager without STT/UI, you can either:</li> <li> <p>Run the built\u2011in chat mode:</p> <pre><code>cd interaction\nconda activate nadine\npython -m nadine --chatmode\n</code></pre> </li> <li> <p>Or call the <code>DialogueManager</code> directly from a small script:</p> <pre><code>from nadine.agents.dm import DialogueManager\n\ndm = DialogueManager()\nwhile True:\n    text = input(\"You: \")\n    if text.lower() == \"exit\":\n        break\n    reply = dm.processInput(text)\n    print(\"Nadine:\", reply)\n</code></pre> </li> <li> <p>Use the Agents &amp; Graph and Memory &amp; RAG pages to understand how state fields like <code>user_info</code>, <code>conversation_memory</code>, <code>episode_memory</code>, and <code>visual_memory</code> are set and used.</p> </li> </ul>"},{"location":"overview/","title":"Nadine - Humanoid Social Robot System","text":"<p>Nadine is a humanoid social robot developed at MIRALab, University of Geneva, under the direction of Professor Nadia Magnenat-Thalmann. This system enables natural, personalized, and emotionally intelligent human-robot interactions through advanced AI agents, computer vision, and multi-modal communication.</p>"},{"location":"overview/#overview","title":"Overview","text":"<p>Nadine is designed as a companion, assistant, and research collaborator that can:</p> <ul> <li>Recognize and remember people through face recognition  </li> <li>Hold context-aware, multi-language conversations  </li> <li>Display emotions and maintain natural eye contact  </li> <li>Access knowledge through Retrieval-Augmented Generation (RAG)  </li> <li>Store and retrieve personalized user memories  </li> <li>Process visual information and understand scenes  </li> <li>Control physical movements and animations  </li> </ul>"},{"location":"overview/#how-this-documentation-is-organized","title":"How This Documentation Is Organized","text":"<p>The rest of the Project Overview section is split across multiple pages:</p> <ul> <li>Installation &amp; Setup \u2013 hardware/software prerequisites, conda environments, MQTT broker, models.  </li> <li>Usage \u2013 how to start Nadine (quick start and per-component commands).  </li> <li>Architecture &amp; Features \u2013 system components, workflow, project structure, and key features.  </li> <li>Development &amp; Troubleshooting \u2013 configuration, MQTT topic summary, developer tips, and troubleshooting.</li> </ul> <p>Each layer (Interaction, Control, Perception) also has its own detailed documentation section in the left navigation.</p>"},{"location":"perception-memory/","title":"Perception Layer \u2013 Selective Memory","text":"<p>This page focuses on the Selective Memory Module (<code>selective_memory.py</code>), which decides which visual scenes are memorable enough to store for each user.</p>"},{"location":"perception-memory/#purpose","title":"Purpose","text":"<p>For every recognized user, the system occasionally evaluates the current visual scene and computes a memorability score based on:</p> <ul> <li>How emotionally salient the scene is  </li> <li>How novel the scene is compared to past scenes for that user  </li> </ul> <p>If the memorability score is high enough, the scene is stored (image + embedding + metadata) in the user\u2019s memory folder for later use by the interaction layer.</p>"},{"location":"perception-memory/#components","title":"Components","text":"<p><code>SelectiveMemoryModule</code> combines three main pieces:</p> <ul> <li>Emotion analysis (OpenFace)</li> <li>Uses OpenFace (<code>MultitaskPredictor</code> + <code>FaceDetector</code>) to infer probabilities over:<ul> <li><code>['neutral', 'happy', 'sad', 'surprise', 'fear', 'disgust', 'anger', 'contempt']</code></li> </ul> </li> <li> <p>Produces a normalized probability distribution over these emotions.</p> </li> <li> <p>Scene embedding (CLIP)</p> </li> <li>Uses <code>openai/clip-vit-large-patch14</code> to embed the entire RGB frame into a high\u2011dimensional vector.</li> <li> <p>Embeddings are (L_2)-normalized for cosine similarity comparisons.</p> </li> <li> <p>Scene description (optional VLM)</p> </li> <li>Optionally uses <code>moondream2</code> to generate a short textual description for each stored scene.</li> <li>If the model is unavailable, description generation is skipped gracefully.</li> </ul> <p>All models are loaded once at initialization and run on the best available device (CUDA/MPS/CPU).</p>"},{"location":"perception-memory/#emotion-novelty-to-memorability","title":"Emotion &amp; Novelty to Memorability","text":""},{"location":"perception-memory/#emotion-salience","title":"Emotion salience","text":"<p>Given emotion probabilities (p_e) and thresholds (t_e) per emotion, salience is computed as:</p> <p>[ \\text{salience}(p_e, t_e) = \\max\\left(0, \\frac{p_e - t_e}{1 - t_e}\\right) ]</p> <ul> <li>Emotion thresholds are tuned per emotion (e.g., lower thresholds for \u201chappy/sad\u201d, higher for \u201csurprise/disgust\u201d).  </li> <li>The final emotion salience is the maximum salience value across active emotions:</li> <li>Active set: <code>['happy', 'sad', 'surprise', 'fear', 'disgust', 'anger', 'contempt']</code>.</li> </ul>"},{"location":"perception-memory/#novelty","title":"Novelty","text":"<p>Novelty is based on distance from past scenes stored for the same user:</p> <ol> <li>Compute CLIP embedding for the current frame.  </li> <li>Load all previous embeddings for that user from:</li> <li><code>interaction/db/memory/user_profiles/&lt;user_id&gt;/memorable_scenes/embeddings/</code></li> <li>Compute cosine similarity between the current embedding and all stored embeddings:</li> <li>Distance = (1 - \\text{similarity})</li> <li>Define novelty score as the minimum distance (closest match); higher = more novel.</li> </ol> <p>Novelty salience is then computed using the same threshold-based mapping as emotions:</p> <p>[ \\text{novelty_salience} = \\max\\left(0, \\frac{\\text{novelty_score} - t_{\\text{novelty}}}{1 - t_{\\text{novelty}}}\\right) ]</p> <p>Where (t_{\\text{novelty}}) is <code>novelty_threshold</code> from config.</p>"},{"location":"perception-memory/#combined-memorability","title":"Combined memorability","text":"<p>Memorability is a weighted sum of emotion and novelty salience:</p> <p>[ \\text{memorability} = w_{\\text{emotion}} \\cdot \\text{emotion_salience}                      + w_{\\text{novelty}} \\cdot \\text{novelty_salience} ]</p> <ul> <li>Weights <code>w_emotion</code> and <code>w_novelty</code> are configured in <code>perception/config.yaml</code>.  </li> <li>If <code>memorability</code> \u2265 <code>memorability_threshold</code>, the scene is stored.</li> </ul>"},{"location":"perception-memory/#storage-layout","title":"Storage Layout","text":"<p>For each user, memorable scenes are stored under:</p> <p><code>interaction/db/memory/user_profiles/&lt;user_id&gt;/memorable_scenes/</code></p> <p>Within that directory:</p> <ul> <li><code>images/</code> </li> <li>Raw RGB images (no bounding boxes drawn).  </li> <li> <p>Filename: <code>scene_YYYYMMDD_HHMMSS.jpg</code></p> </li> <li> <p><code>embeddings/</code> </p> </li> <li>Numpy <code>.npy</code> files with CLIP embeddings.  </li> <li> <p>Filename: <code>scene_YYYYMMDD_HHMMSS_embedding.npy</code></p> </li> <li> <p><code>metadata/</code> </p> </li> <li>JSON files with metadata, including:<ul> <li><code>scene_id</code>, <code>timestamp</code>, <code>image_path</code></li> <li><code>memorability</code>, <code>emotion_salience</code>, <code>novelty_salience</code></li> <li>Full <code>emotion_probs</code></li> <li>Optional natural language <code>description</code></li> </ul> </li> <li>Filename: <code>scene_YYYYMMDD_HHMMSS_metadata.json</code></li> </ul> <p>This structure lets downstream components (e.g., memory/RAG agents) quickly discover and use memorable scenes.</p>"},{"location":"perception-memory/#configuration-hooks","title":"Configuration Hooks","text":"<p>You can tune selective memory behavior via <code>perception/config.yaml</code>:</p> <ul> <li><code>w_emotion</code>, <code>w_novelty</code> \u2013 balance between emotional and novelty cues.  </li> <li><code>novelty_threshold</code> \u2013 how easily novelty becomes salient.  </li> <li><code>memorability_threshold</code> \u2013 how \u201cpicky\u201d the system is about storing scenes.  </li> <li><code>memorability_check_interval</code> (in <code>main.py</code>) \u2013 how often memorability is evaluated per user.</li> </ul> <p>For finer control (code changes), you can adjust:</p> <ul> <li>Per\u2011emotion thresholds in <code>SelectiveMemoryModule.emotion_thresholds</code>.  </li> <li>The way novelty is normalized or aggregated if you want more sophisticated behavior.</li> </ul>"},{"location":"perception-overview/","title":"Perception Layer \u2013 Overview","text":"<p>The perception layer senses and interprets the visual scene around Nadine.</p> <p>If you are new to the project, start with Project Overview, then use this page to understand what the perception component does and how to run it.</p>"},{"location":"perception-overview/#responsibilities","title":"Responsibilities","text":"<ul> <li>Capture RGB\u2011D frames from the Intel RealSense camera  </li> <li>Detect and track faces using YOLOv8 </li> <li>Recognize known users with InsightFace and a local face database  </li> <li>Publish user position and identity via MQTT to other layers  </li> <li>Trigger visual memory updates, delegating memorability decisions to the selective-memory module</li> </ul>"},{"location":"perception-overview/#files-and-modules","title":"Files and Modules","text":"<p>Main perception files under <code>perception/</code>:</p> <ul> <li><code>main.py</code>: main runtime loop (camera, detection, recognition, MQTT, selective memory integration).</li> <li><code>config.yaml</code>: configuration for MQTT, YOLO face model, and selective memory (weights/thresholds).</li> <li><code>selective_memory.py</code>: <code>SelectiveMemoryModule</code> class for computing memorability and writing memorable scenes.</li> <li><code>utils.py</code>: logging (<code>LoggersFactory</code>) and <code>user_info_init</code> helper.</li> <li><code>run.sh</code>: activates the <code>nadine</code> conda env and runs <code>main.py</code>.</li> <li><code>models/</code> and <code>weights/</code>: model checkpoints for YOLO and OpenFace/OpenFace\u2011based models.</li> </ul>"},{"location":"perception-overview/#how-to-run","title":"How to Run","text":""},{"location":"perception-overview/#prerequisites","title":"Prerequisites","text":"<ul> <li>Environment: <code>nadine</code> conda environment (from <code>interaction/environment.yml</code>).  </li> <li>Hardware: Intel RealSense RGB\u2011D camera connected and accessible.  </li> <li>Services: MQTT broker at <code>localhost</code> or <code>emqx</code> (default ports).  </li> <li>Data layout: interaction DB at <code>interaction/db/memory/user_profiles/</code> (created automatically as needed).</li> </ul>"},{"location":"perception-overview/#start-command","title":"Start command","text":"<p>From the project root:</p> <pre><code>cd /home/miralab/Development/nadine_Jan_2026\nbash perception/run.sh\n</code></pre> <p>This will:</p> <ul> <li>Activate <code>nadine</code> conda env  </li> <li>Run <code>python3 perception/main.py</code> </li> <li>Start RealSense, YOLO, InsightFace, MQTT, and selective memory integration</li> </ul> <p>To stop, press <code>q</code> in the OpenCV window or interrupt the process with <code>Ctrl+C</code>.</p>"},{"location":"perception-overview/#configuration-perceptionconfigyaml","title":"Configuration (<code>perception/config.yaml</code>)","text":"<p>Perception\u2011specific configuration lives under the <code>perception:</code> key:</p> <ul> <li><code>mqtt</code></li> <li><code>primary_host</code>, <code>fallback_host</code>, <code>port</code>, <code>keepalive</code></li> <li><code>yolo_face</code></li> <li><code>model_path</code> (default <code>models/yolov8n-face.pt</code>)</li> <li><code>confidence</code> (detection threshold)</li> <li><code>selective_memory</code></li> <li><code>w_emotion</code>, <code>w_novelty</code></li> <li><code>novelty_threshold</code></li> <li><code>memorability_threshold</code></li> <li><code>memorability_check_interval</code></li> </ul> <p><code>main.py</code> loads this via <code>load_perception_config()</code> into <code>_mqtt_cfg</code>, <code>_yolo_cfg</code>, and <code>_sm_cfg</code>.</p>"},{"location":"perception-overview/#where-to-go-next","title":"Where to Go Next","text":"<ul> <li>See Perception Layer / Runtime &amp; MQTT for a step\u2011by\u2011step view of the frame loop and topic usage.  </li> <li>See Perception Layer / Selective Memory for details on memorability computation and scene storage.</li> </ul>"},{"location":"perception-runtime/","title":"Perception Layer \u2013 Runtime &amp; MQTT","text":"<p>This page describes how the perception runtime loop works and how it communicates with other layers over MQTT.</p>"},{"location":"perception-runtime/#initialization-mainpy","title":"Initialization (<code>main.py</code>)","text":"<p>On startup, <code>main.py</code>:</p> <ul> <li>Logging &amp; paths</li> <li>Initializes a shared logger via <code>LoggersFactory.getLogger()</code>.</li> <li> <p>Resolves <code>base_dir</code> (project root) and <code>USER_PROFILES_DIR</code> (<code>interaction/db/memory/user_profiles/</code>).</p> </li> <li> <p>Configuration</p> </li> <li>Loads <code>perception/config.yaml</code> via <code>load_perception_config()</code>.</li> <li> <p>Extracts <code>_mqtt_cfg</code>, <code>_yolo_cfg</code>, <code>_sm_cfg</code> for MQTT, YOLO, and selective memory.</p> </li> <li> <p>MQTT client</p> </li> <li>Creates a <code>paho.mqtt.client.Client</code>.</li> <li>Connects to <code>_mqtt_cfg.primary_host</code> or <code>_mqtt_cfg.fallback_host</code>.</li> <li> <p>Subscribes to:</p> <ul> <li><code>nadine/face_recognition/user_info</code> \u2013 user info for face storage.</li> <li><code>nadine/perception/capture_current_view</code> \u2013 request for a one\u2011shot camera snapshot.</li> </ul> </li> <li> <p>RealSense camera</p> </li> <li>Starts a RealSense <code>pipeline</code> with:<ul> <li>Color stream: <code>640x480</code>, 30 fps, BGR.</li> <li>Depth stream: <code>640x480</code>, 30 fps.</li> </ul> </li> <li> <p>Uses <code>rs.align(rs.stream.color)</code> so depth aligns with color.</p> </li> <li> <p>Models</p> </li> <li>YOLO face detector: <code>model_face = YOLO(_yolo_cfg[\"model_path\"], ...)</code>.</li> <li> <p>InsightFace: <code>FaceAnalysis</code> with detection + recognition.</p> </li> <li> <p>Face database</p> </li> <li> <p>Calls <code>load_known_faces(face_analyzer)</code>:</p> <ul> <li>Iterates over each user in <code>USER_PROFILES_DIR</code>.</li> <li>Loads existing face embeddings or derives them from stored face images.</li> </ul> </li> <li> <p>Selective memory</p> </li> <li>Creates a <code>SelectiveMemoryModule</code> with:<ul> <li>OpenFace backbone and face detector weights (<code>perception/weights/</code>).</li> <li>Weights/thresholds from <code>_sm_cfg</code> (emotion, novelty, memorability).</li> </ul> </li> </ul>"},{"location":"perception-runtime/#per-frame-loop","title":"Per-Frame Loop","text":"<p>The main <code>while True</code> loop does, for each frame:</p> <ol> <li>Capture frames</li> <li>Wait for frames from RealSense.</li> <li> <p>Align depth to color and extract:</p> <ul> <li><code>color_frame</code> \u2192 <code>color_image</code> (BGR <code>numpy</code> array).</li> <li><code>depth_frame</code> \u2192 <code>depth_image</code> (depth map).</li> </ul> </li> <li> <p>YOLO face detection &amp; tracking</p> </li> <li>Run <code>model_face.track(color_image, ...)</code>.</li> <li> <p>For each detected face, collect:</p> <ul> <li>Bounding box <code>(x1, y1, x2, y2)</code>.</li> <li>Tracker ID.</li> <li>Depth at the center of the box from <code>depth_frame</code>.</li> </ul> </li> <li> <p>Active user selection</p> </li> <li>Among all detected faces, pick the one with minimum positive depth as the \u201cactive user\u201d.</li> <li> <p>Maintain <code>tracked_user_id</code> and only update it if:</p> <ul> <li>The closest face changes, or</li> <li>More than ~2.5 seconds have passed since the last update.</li> </ul> </li> <li> <p>Face recognition</p> </li> <li> <p>If a <code>tracked_user_id</code> is set:</p> <ul> <li>Run InsightFace detection (<code>face_analyzer.get(color_image)</code>).</li> <li>For each InsightFace detection, compute IoU with the YOLO bbox of the tracked user.</li> <li>Pick the best IoU match and, if an embedding is available:</li> <li>Compute cosine similarities to all <code>known_embeddings</code>.</li> <li>If the best similarity passes threshold, set:<ul> <li><code>name</code> (user name),</li> <li><code>user_id</code>,</li> <li><code>confidence</code> (%).</li> </ul> </li> </ul> </li> <li> <p>3D position estimation</p> </li> <li>For the tracked face, compute:<ul> <li>Center pixel <code>(cx, cy)</code> from the bounding box.</li> <li>Depth at <code>(cx, cy)</code> from <code>depth_frame</code>.</li> <li>3D coordinates <code>coords_3d = rs2_deproject_pixel_to_point(...)</code>.</li> </ul> </li> <li> <p>If coordinates are non\u2011zero, publish:</p> <ul> <li>Topic: <code>nadine/agent/control/look_at</code> </li> <li>Payload: <code>{\"x\": coords_3d[0], \"y\": coords_3d[1], \"z\": coords_3d[2]}</code></li> </ul> </li> <li> <p>User identity updates</p> </li> <li> <p>Every 5 seconds, or when the recognized name changes, publish:</p> <ul> <li>Topic: <code>nadine/graph/user_detected</code> </li> <li>Payload: <code>{\"user_name\": name, \"confidence\": confidence, \"user_id\": user_id}</code></li> </ul> </li> <li> <p>Selective memory hook</p> </li> <li> <p>For recognized users (<code>user_id</code> not <code>None</code>), periodically (per user):</p> <ul> <li>Compute emotion probabilities from the full frame.</li> <li>Compute a CLIP embedding for scene novelty.</li> <li>Ask <code>SelectiveMemoryModule</code> for a memorability score.</li> <li>If memorability exceeds threshold, store the scene and publish an event (see the Selective Memory page for details).</li> </ul> </li> <li> <p>Visualization</p> </li> <li>Draw the face bounding box, label with <code>name (confidence%)</code> and the 3D coordinates.</li> <li> <p>Show the window <code>\"Face &amp; 3D Tracking\"</code>.</p> </li> <li> <p>Face storage</p> </li> <li>When <code>nadine/face_recognition/user_info</code> is received:<ul> <li>Call <code>store_face_for_user(...)</code> to store image and embeddings for that user.</li> <li>Set a flag to reload <code>known_embeddings</code> on the next loop.</li> </ul> </li> </ol> <p>On exit (key <code>q</code> or exception), the pipeline is stopped, windows are closed, and the MQTT loop is stopped.</p>"},{"location":"perception-runtime/#mqtt-topics-runtime-summary","title":"MQTT Topics (Runtime Summary)","text":"<p>Subscribed</p> <ul> <li><code>nadine/face_recognition/user_info</code> </li> <li>Payload: <code>{\"user_name\": \"...\", \"user_id\": \"...\"}</code> </li> <li> <p>Purpose: instruct perception to store a face image + embedding for this user.</p> </li> <li> <p><code>nadine/perception/capture_current_view</code> </p> </li> <li>Payload: file path string  </li> <li>Purpose: capture a single RGB frame and save it to the given path.</li> </ul> <p>Published</p> <ul> <li><code>nadine/agent/control/look_at</code> </li> <li>Payload: <code>{\"x\": float, \"y\": float, \"z\": float}</code> </li> <li> <p>Used by the control layer to orient Nadine\u2019s head/eyes toward the user.</p> </li> <li> <p><code>nadine/graph/user_detected</code> </p> </li> <li>Payload: <code>{\"user_name\": str, \"confidence\": float, \"user_id\": Optional[str]}</code> </li> <li> <p>Used by the interaction layer to know who is in front of Nadine.</p> </li> <li> <p><code>nadine/graph/face_stored</code> </p> </li> <li>Payload: <code>{\"user_name\": str, \"user_id\": str, \"status\": \"face_stored\"}</code> </li> <li> <p>Emitted after new face data has been persisted.</p> </li> <li> <p><code>nadine/memory/scene_stored</code> </p> </li> <li>Payload: user ID/name, memorability metrics, and optional description  </li> <li>Emitted after a memorable scene is stored by the selective memory module.</li> </ul>"},{"location":"perception-runtime/#utilities","title":"Utilities","text":"<p>Perception reuses utilities from <code>perception/utils.py</code>:</p> <ul> <li><code>LoggersFactory</code> \u2013 central logger for all perception logs.  </li> <li><code>user_info_init</code> \u2013 initializes default <code>user_info.json</code> content for new users.</li> </ul>"},{"location":"project-architecture/","title":"Project Overview \u2013 Architecture &amp; Features","text":"<p>This page gives a high-level view of Nadine\u2019s architecture and major features.</p>"},{"location":"project-architecture/#system-architecture","title":"System Architecture","text":"<p>Nadine is organized into three main components that communicate via MQTT:</p>"},{"location":"project-architecture/#1-control-component-control","title":"1. Control Component (<code>control/</code>)","text":"<p>Handles robot physical control, animations, and speech synthesis:</p> <ul> <li>Robot joint control and animations (XML-based).  </li> <li>Azure Text-to-Speech (TTS) integration.  </li> <li>Serial communication with robot hardware.  </li> <li>Head/eye movement control (look-at functionality).  </li> <li>Lip-sync animation generation.  </li> </ul>"},{"location":"project-architecture/#2-interaction-component-interaction","title":"2. Interaction Component (<code>interaction/</code>)","text":"<p>Multi-agent dialogue system built with LangGraph:</p> <ul> <li>Orchestration Agent \u2013 routes requests to the right sub-agents.  </li> <li>Response Agent \u2013 generates conversational responses with emotional tone.  </li> <li>Search Agent \u2013 real-time web information retrieval.  </li> <li>Knowledge RAG Agent \u2013 retrieves information from Nadine\u2019s knowledge base.  </li> <li>Vision Agent \u2013 interfaces with visual reasoning tools.  </li> <li>Memory Agents \u2013 store and retrieve user-specific information.  </li> <li>Affective System \u2013 maintains emotional state (PAD model).  </li> <li>Dialogue Manager \u2013 coordinates conversation flow.  </li> <li>Speech-to-Text (Google Cloud Speech).  </li> <li>Multi-language support (English, German, French, Chinese).  </li> <li>UI for monitoring.  </li> </ul>"},{"location":"project-architecture/#3-perception-component-perception","title":"3. Perception Component (<code>perception/</code>)","text":"<p>Computer vision and face recognition:</p> <ul> <li>RealSense camera integration (RGB + depth).  </li> <li>Face detection using YOLOv8.  </li> <li>Face recognition using InsightFace.  </li> <li>3D position tracking for look-at control.  </li> <li>User profile management with face embeddings.  </li> </ul>"},{"location":"project-architecture/#system-workflow","title":"System Workflow","text":"<p>The following diagram summarizes how components and agents interact:</p> <p></p> <p>At a high level:</p> <ol> <li>Perception detects and recognizes users, publishes identity and 3D position.  </li> <li>Interaction receives user speech and visual context, runs the multi-agent graph.  </li> <li>Control executes speech and animations based on interaction commands.  </li> </ol> <p>See the dedicated Perception, Interaction, and Control layer docs for details.</p>"},{"location":"project-architecture/#project-structure","title":"Project Structure","text":"<p>Simplified directory layout:</p> <pre><code>nadine_Jan_2026/\n\u251c\u2500\u2500 control/                 # Robot control and animations\n\u2502   \u251c\u2500\u2500 main.py             # Control server entry point\n\u2502   \u251c\u2500\u2500 nadine/control/    # Control modules\n\u2502   \u2514\u2500\u2500 XMLAnimations/      # Animation XML files\n\u2502\n\u251c\u2500\u2500 interaction/            # Multi-agent dialogue system\n\u2502   \u251c\u2500\u2500 nadine/agents/     # LangGraph agents\n\u2502   \u251c\u2500\u2500 nadine/common/     # MQTT, translation, logging\n\u2502   \u251c\u2500\u2500 nadine/stt/        # Speech-to-text\n\u2502   \u2514\u2500\u2500 db/                # Databases (memory, knowledge, images)\n\u2502\n\u251c\u2500\u2500 perception/            # Computer vision\n\u2502   \u251c\u2500\u2500 main.py            # Face recognition main loop\n\u2502   \u2514\u2500\u2500 models/            # ML models\n\u2502\n\u251c\u2500\u2500 compose.yaml           # Docker Compose for MQTT\n\u2514\u2500\u2500 start_nadine.sh        # Main startup script\n</code></pre>"},{"location":"project-architecture/#key-features","title":"Key Features","text":""},{"location":"project-architecture/#multi-agent-system-interaction-layer","title":"Multi-Agent System (Interaction Layer)","text":"<p>The interaction component uses LangGraph to orchestrate multiple specialized agents:</p> <ul> <li>Orchestration \u2013 decides which agents to call.  </li> <li>Search \u2013 real-time web information retrieval.  </li> <li>Knowledge RAG \u2013 access to Nadine\u2019s internal knowledge base.  </li> <li>Vision \u2013 image understanding and scene analysis.  </li> <li>Memory \u2013 persistent user profile and episodic memory management.  </li> <li>Response \u2013 context-aware, emotionally intelligent replies.  </li> </ul>"},{"location":"project-architecture/#face-recognition-gaze-perception-control","title":"Face Recognition &amp; Gaze (Perception + Control)","text":"<ul> <li>Real-time face detection and tracking.  </li> <li>User identification with confidence scoring.  </li> <li>Automatic face storage for new users.  </li> <li>3D position tracking to drive natural gaze and head movements.  </li> </ul>"},{"location":"project-architecture/#multi-language-support","title":"Multi-Language Support","text":"<ul> <li>Automatic language detection and translation.  </li> <li>Language-specific TTS voices.  </li> <li>Currently supports: English, German, French, Chinese (with flexibility to extend).  </li> </ul>"},{"location":"project-architecture/#memory-system","title":"Memory System","text":"<ul> <li>User profile management (<code>user_info.json</code> per user).  </li> <li>Conversation history and episodic memories stored in ChromaDB.  </li> <li>Visual memory (memorable scenes) linked to user IDs.  </li> </ul>"},{"location":"project-architecture/#affective-system","title":"Affective System","text":"<ul> <li>PAD (Pleasure\u2013Arousal\u2013Dominance) emotional model.  </li> <li>Emotion-aware response generation.  </li> <li>Dynamic mood and emotion updates per interaction.  </li> </ul>"},{"location":"project-architecture/#multimodal-memory-framework","title":"Multimodal Memory Framework","text":"<p>Nadine implements a multimodal memory framework that tightly couples perception and interaction:</p> <ul> <li>Selective visual memory (perception) </li> <li>The perception layer computes a memorability score per frame using:<ul> <li>Emotion salience from OpenFace (facial expressions).  </li> <li>Novelty from CLIP embeddings vs. past scenes for that user.  </li> </ul> </li> <li> <p>Only scenes above a configurable threshold are stored under each user\u2019s profile as:</p> <ul> <li>RGB images, CLIP embeddings, and JSON metadata (emotions, memorability, optional scene description).  </li> </ul> </li> <li> <p>Textual &amp; episodic memory (interaction) </p> </li> <li> <p>The interaction layer stores:</p> <ul> <li>Conversation snippets and episodic summaries in a per-user Chroma collection.  </li> <li>Structured user profiles in <code>user_info.json</code> and <code>user_ids.json</code>.  </li> </ul> </li> <li> <p>Hybrid retrieval (interaction) </p> </li> <li>For a new user query, the memory agents:<ul> <li>Query Chroma for the most relevant conversation and episode documents.  </li> <li>Use CLIP text embeddings to find the most relevant visual memory (image) for that user.  </li> <li>Apply configurable thresholds and weights (image vs. description similarity) from <code>interaction/config.yaml</code>.  </li> </ul> </li> <li>The best matching visual memory (if any) is passed to the response LLM as an attached image, alongside textual memories and RAG results.</li> </ul> <p>Together, this allows Nadine\u2019s responses to be grounded in what was seen, what was said, and who the user is, rather than relying on text alone.</p> <p></p>"},{"location":"project-architecture/#research-applications","title":"Research &amp; Applications","text":"<p>Nadine has been used in various research and real-world contexts:</p> <ul> <li>Customer service (AIA Singapore).  </li> <li>Elderly care and companionship.  </li> <li>Public demonstrations and conferences.  </li> <li>Human-robot interaction research.  </li> <li>AI-for-Good initiatives.  </li> </ul>"},{"location":"project-dev/","title":"Project Overview \u2013 Development &amp; Troubleshooting","text":"<p>This page collects development notes, configuration hints, MQTT topics, and troubleshooting tips.</p>"},{"location":"project-dev/#configuration","title":"Configuration","text":""},{"location":"project-dev/#environment-variables","title":"Environment Variables","text":"<p>Create <code>.env</code> files in the relevant component directories with:</p> <ul> <li>API keys for Azure Text-to-Speech and Google Cloud services.  </li> <li>LLM configuration (e.g. Ollama endpoint, API keys).  </li> <li>MQTT broker settings (if different from defaults).  </li> </ul>"},{"location":"project-dev/#agent-toggles","title":"Agent Toggles","text":"<p>Control which agents are active via environment variables:</p> <ul> <li><code>NADINE_ENABLE_VISION_AGENT</code> \u2013 enable/disable vision agent.  </li> <li><code>NADINE_ENABLE_MEMORY_AGENTS</code> \u2013 enable/disable memory agents.  </li> </ul> <p>These are read by <code>graph.py</code> to skip certain agents when running on resource-constrained systems.</p>"},{"location":"project-dev/#mqtt-topics-summary","title":"MQTT Topics (Summary)","text":"<p>The system uses MQTT for inter-component communication.</p> <p>Perception -&gt; Interaction</p> <ul> <li><code>nadine/graph/user_detected</code> \u2013 user recognition events.  </li> <li><code>nadine/graph/face_stored</code> \u2013 face storage confirmations.  </li> </ul> <p>Interaction -&gt; Control</p> <ul> <li><code>nadine/agent/control/speak</code> \u2013 text-to-speech requests.  </li> <li><code>nadine/agent/control/look_at</code> \u2013 gaze (3D position) control commands.  </li> <li><code>nadine/agent/control/animation</code> \u2013 animation requests.  </li> </ul> <p>Perception -&gt; Control</p> <ul> <li><code>nadine/agent/control/look_at</code> \u2013 3D position for gaze.  </li> </ul> <p>Interaction -&gt; Perception</p> <ul> <li><code>nadine/face_recognition/user_info</code> \u2013 user information for face storage.  </li> <li><code>nadine/perception/capture_current_view</code> \u2013 request the current camera view.  </li> </ul> <p>For topic-by-topic payload details, see the individual Perception, Interaction, and Control layer docs.</p>"},{"location":"project-dev/#development-tips","title":"Development Tips","text":""},{"location":"project-dev/#testing-agents","title":"Testing Agents","text":"<p>All agents include test harnesses that can be run directly:</p> <pre><code>cd interaction/nadine/agents\n\n# Test intent classification\npython intention_classifier_test.py\n\n# Test memory update agent\npython memory_update_agent.py\n\n# Test orchestration agent\npython orchestration_agent.py\n\n# Test search agent\npython search_agent.py\n\n# Test vision agent\npython vision_agent.py\n\n# Test response agent\npython response_agent.py\n\n# Test contextualizer\npython context_summarizer.py\n</code></pre> <p>Test Samples: All test cases are centralized in <code>agent_test_samples.py</code> for easy maintenance. When adding new test cases, add them to this file and import them in the respective agent test harnesses.</p>"},{"location":"project-dev/#adding-new-agents-interaction-layer","title":"Adding New Agents (Interaction Layer)","text":"<ol> <li>Create a new agent function under <code>interaction/nadine/agents/</code>.  </li> <li>Add the agent as a node in <code>interaction/nadine/agents/graph.py</code>.  </li> <li>Update the orchestration agent's prompt and parsing logic if necessary.  </li> <li>Wire the agent into the graph using <code>add_node</code> and <code>add_conditional_edges</code>.</li> <li>Add test cases to <code>agent_test_samples.py</code> and create a test harness in the agent file.  </li> </ol>"},{"location":"project-dev/#extending-the-knowledge-base","title":"Extending the Knowledge Base","text":"<ul> <li>Add documents to <code>interaction/db/knowledge/rag_files/</code>.  </li> <li>They will be indexed automatically by the knowledge RAG pipeline on first use.  </li> </ul>"},{"location":"project-dev/#custom-animations-control-layer","title":"Custom Animations (Control Layer)","text":"<ul> <li>Add XML animation files to <code>control/XMLAnimations/</code> following existing examples.  </li> <li>Map them from logical names in <code>AgentControlHandler</code> or in higher-level interaction logic.  </li> </ul>"},{"location":"project-dev/#troubleshooting","title":"Troubleshooting","text":""},{"location":"project-dev/#mqtt-connection-issues","title":"MQTT Connection Issues","text":"<ul> <li>Ensure Docker Compose services are running:</li> </ul> <pre><code>docker compose ps\n</code></pre> <ul> <li>Check EMQX broker logs:</li> </ul> <pre><code>docker compose logs emqx\n</code></pre> <ul> <li>Verify that ports <code>1883</code> (MQTT) and <code>18083</code> (dashboard) are not blocked.</li> </ul>"},{"location":"project-dev/#face-recognition-not-working","title":"Face Recognition Not Working","text":"<ul> <li>Check RealSense camera connection and permissions.  </li> <li>Confirm that required models are downloaded and accessible.  </li> <li>Look at <code>perception/logs/</code> for detailed errors.  </li> </ul>"},{"location":"project-dev/#audio-issues","title":"Audio Issues","text":"<ul> <li>Check PulseAudio / system audio configuration (see <code>start_nadine.sh</code>).  </li> <li>Verify microphone and speaker permissions.  </li> <li>Test with <code>--nomqtt</code> to isolate interaction from control/perception issues.  </li> </ul>"},{"location":"project-dev/#llm-errors","title":"LLM Errors","text":"<ul> <li>Verify that Ollama (or your LLM backend) is running.  </li> <li>Check API keys in <code>.env</code>.  </li> <li>Inspect logs in <code>interaction/logs/</code> and agent-specific logs if enabled.</li> <li>Invalid JSON Output: If agents return invalid JSON, check:</li> <li>Prompt length (long prompts can cause issues with small LLMs).</li> <li>Use the test harnesses to debug specific agents.</li> <li>Check that <code>format=\"json\"</code> is set in <code>load_agent_llm()</code> calls where needed.</li> </ul>"},{"location":"project-dev/#prompt-optimization","title":"Prompt Optimization","text":"<p>All agent prompts have been optimized for small LLMs (e.g., <code>granite4:350m</code>, <code>qwen2.5:1.5b-instruct</code>) to improve: - Speed: Shorter, more directive prompts reduce latency. - Accuracy: Explicit rules and examples guide the LLM to correct outputs. - JSON Output: Prompts explicitly require JSON format with examples. - Consistency: Clear rules reduce variability in outputs.</p> <p>Key optimization strategies: - Use explicit, numbered rules instead of verbose explanations. - Include concrete examples in the prompt. - Emphasize critical constraints (e.g., \"NEVER answer\", \"Output JSON only\"). - Remove unnecessary context that doesn't affect the task. - Use <code>format=\"json\"</code> parameter in <code>load_agent_llm()</code> for structured outputs.</p> <p>When modifying prompts, test with the agent's test harness to ensure performance is maintained.  </p>"},{"location":"project-dev/#logging","title":"Logging","text":"<p>Logs are stored in:</p> <ul> <li><code>control/logs/</code> \u2013 control component logs.  </li> <li><code>interaction/logs/</code> \u2013 interaction component logs.  </li> <li><code>perception/logs/</code> \u2013 perception component logs.  </li> <li><code>mqtt-monitor-logs/</code> \u2013 MQTT monitoring logs.  </li> </ul> <p>Central loggers (<code>LoggersFactory</code>) provide consistent formatting across components.</p>"},{"location":"project-dev/#acknowledgments","title":"Acknowledgments","text":"<p>Nadine is developed at MIRALab, University of Geneva, under the direction of Professor Nadia Magnenat-Thalmann.</p>"},{"location":"project-installation/","title":"Project Overview \u2013 Installation &amp; Setup","text":"<p>This page explains what you need to install and configure before running Nadine.</p>"},{"location":"project-installation/#prerequisites","title":"Prerequisites","text":""},{"location":"project-installation/#hardware","title":"Hardware","text":"<ul> <li>Intel RealSense camera (D400 series recommended)  </li> <li>Microphone for speech input  </li> <li>Speakers for audio output  </li> </ul>"},{"location":"project-installation/#software","title":"Software","text":"<ul> <li>Python 3.10  </li> <li>Conda (Anaconda/Miniconda)  </li> <li>Docker and Docker Compose (for the MQTT broker)  </li> <li>CUDA-capable GPU (recommended for faster inference)  </li> </ul>"},{"location":"project-installation/#api-keys-and-services","title":"API Keys and Services","text":"<p>Create a <code>.env</code> file in the <code>interaction/</code> directory with:</p> <ul> <li>Azure TTS credentials  </li> <li>Google Cloud Speech API credentials  </li> <li>Google Cloud Translate API credentials  </li> <li>LLM API keys / configuration (e.g. for Ollama or other providers)  </li> </ul>"},{"location":"project-installation/#conda-environments","title":"Conda Environments","text":""},{"location":"project-installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone &lt;repository-url&gt;\ncd nadine_Jan_2026\n</code></pre>"},{"location":"project-installation/#2-set-up-conda-environments","title":"2. Set Up Conda Environments","text":"<p>Control component</p> <pre><code>cd control\nconda env create -f environment.yml\nconda activate nadine_new\n</code></pre> <p>Interaction component</p> <pre><code>cd interaction\nconda env create -f environment.yml\nconda activate nadine\n# Or install via pip:\npip install -r requirements.txt\n</code></pre> <p>Perception component</p> <pre><code>cd perception\nconda activate nadine  # Uses same environment as interaction\n</code></pre>"},{"location":"project-installation/#mqtt-broker-emqx","title":"MQTT Broker (EMQX)","text":"<p>From the project root:</p> <pre><code>docker compose up -d\n</code></pre> <p>The broker will be available at:</p> <ul> <li>MQTT: <code>localhost:1883</code> </li> <li>Dashboard: <code>http://localhost:18083</code> (username: <code>admin</code>, password: <code>public</code>)</li> </ul>"},{"location":"project-installation/#models","title":"Models","text":"<p>On first run, the system will automatically download required models. You can also pre-download:</p> <ul> <li>YOLOv8 face detection model (already placed in <code>perception/models/</code>).  </li> <li>InsightFace models (downloaded automatically on first use).  </li> <li>LLM models (configured via environment variables / Ollama / other backends).  </li> </ul>"},{"location":"project-installation/#environment-file-interactionenv","title":"Environment File (<code>interaction/.env</code>)","text":"<p>Before running the interaction layer, create an <code>.env</code> file in the <code>interaction/</code> directory. This file should contain the API keys and configuration for all external services used by Nadine:</p> <pre><code># OpenAI / Azure OpenAI\nOPENAI_API_KEY=&lt;your_openai_key&gt;              # If you call OpenAI models directly\nazure_openai_endpoint=&lt;https://...&gt;.openai.azure.com/\nazure_openai_api_key=&lt;your_azure_openai_key&gt;\n\n# Google Cloud (STT, Translate, etc.)\nGOOGLE_APPLICATION_CREDENTIALS=/absolute/path/to/google-credentials.json\nGOOGLE_CLOUD_PROJECT=&lt;your_gcp_project_id&gt;\n\n# Web search (Serper)\nSERPER_API_KEY=&lt;your_serper_api_key&gt;\n\n# LangChain / LangGraph observability\nLANGCHAIN_TRACING_V2=true\nLANGCHAIN_API_KEY=&lt;your_langsmith_or_langchain_api_key&gt;\n</code></pre> <p>Notes:</p> <ul> <li><code>GOOGLE_APPLICATION_CREDENTIALS</code> should point to a JSON service-account key file with access to the required Google APIs.  </li> <li>If you are only using Azure OpenAI (and not OpenAI\u2019s public API), you can leave <code>OPENAI_API_KEY</code> unset.  </li> <li>If you don\u2019t want to use LangChain/LangGraph tracing, you can omit <code>LANGCHAIN_TRACING_V2</code> and <code>LANGCHAIN_API_KEY</code>.  </li> </ul>"},{"location":"project-usage/","title":"Project Overview \u2013 Usage","text":"<p>This page explains how to start Nadine, both via the main script and per-component.</p>"},{"location":"project-usage/#quick-start-start_nadinesh","title":"Quick Start (<code>start_nadine.sh</code>)","text":"<p>From the project root:</p> <pre><code>./start_nadine.sh\n</code></pre> <p>This script will:</p> <ul> <li>Launch the control component (robot control server).  </li> <li>Launch the interaction component (dialogue system).  </li> <li>Launch the perception component (face recognition).  </li> </ul> <p>Make sure:</p> <ul> <li>Conda environments are created and available.  </li> <li>EMQX MQTT broker is running (see Installation &amp; Setup).  </li> </ul>"},{"location":"project-usage/#startup-options","title":"Startup Options","text":"<p><code>start_nadine.sh</code> supports several flags:</p> <pre><code># Disable vision agent in the interaction graph\n./start_nadine.sh --no-vision-agent\n\n# Disable memory agents\n./start_nadine.sh --no-memory-agents\n\n# Skip the perception process (no face recognition)\n./start_nadine.sh --no-vision-process\n</code></pre> <p>These are useful for debugging or running Nadine on systems without a camera or without GPU resources for vision.</p>"},{"location":"project-usage/#manual-component-startup","title":"Manual Component Startup","text":"<p>You can also start each component separately.</p>"},{"location":"project-usage/#control-component","title":"Control Component","text":"<pre><code>cd control\nconda activate nadine_new\n./run.sh\n\n# Or with custom paths:\npython main.py -voicepath &lt;path&gt; -animationXMLPath &lt;path&gt;\n</code></pre>"},{"location":"project-usage/#interaction-component","title":"Interaction Component","text":"<pre><code>cd interaction\nconda activate nadine\n./run.sh\n\n# Or run without MQTT:\npython -m nadine --nomqtt\n\n# Or in chat mode (text-only only, no audio/UI):\npython -m nadine --chatmode\n</code></pre>"},{"location":"project-usage/#perception-component","title":"Perception Component","text":"<pre><code>cd perception\nconda activate nadine\n./run.sh\n</code></pre> <p>Each component will log its status to its own <code>logs/</code> folder and communicate over MQTT as described in the layer-specific docs.</p>"}]}